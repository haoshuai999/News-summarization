{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Final_project_biLSTM_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE97AG7KqHA_",
        "colab_type": "text"
      },
      "source": [
        "Applied Deep Learning Final project Fall 2019:\n",
        "\n",
        "*   Shuai Hao (sh3831)\n",
        "*   Bhaskar Ghosh (bg2625)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OY44kqkcv1u0"
      },
      "source": [
        "## Model training and evaluation\n",
        "In this notebook, we further process the data by truncating and padding the sequences. And we define, train and evaluate our bidirectional LSTM model. HPC doesn't support the lastest version of tensorflow, so there are a few warnings in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmxwyECbJTpT",
        "colab_type": "text"
      },
      "source": [
        "Install graphviz, rouge and pydot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99fYcGMqJOHi",
        "colab_type": "code",
        "outputId": "67702eea-3229-44ce-d5a8-741579868795",
        "colab": {}
      },
      "source": [
        "!pip install graphviz --user"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied (use --upgrade to upgrade): graphviz in ./.local/lib/python3.5/site-packages\n",
            "\u001b[33mYou are using pip version 8.1.2, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKO3y1LsJOH5",
        "colab_type": "code",
        "outputId": "cc46d84b-655f-4499-c2a0-21627eef6d6a",
        "colab": {}
      },
      "source": [
        "!pip install rouge --user"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied (use --upgrade to upgrade): rouge in ./.local/lib/python3.5/site-packages\n",
            "\u001b[33mYou are using pip version 8.1.2, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-5F8UVJOIG",
        "colab_type": "code",
        "outputId": "4ecebd87-8ed8-4e3f-9699-0a3d632cb773",
        "colab": {}
      },
      "source": [
        "!pip install pydot --user"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied (use --upgrade to upgrade): pydot in ./.local/lib/python3.5/site-packages\n",
            "Requirement already satisfied (use --upgrade to upgrade): pyparsing>=2.1.4 in /rigel/opt/anaconda3-4.2.0/lib/python3.5/site-packages (from pydot)\n",
            "\u001b[33mYou are using pip version 8.1.2, however version 19.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BpxhPU0KM2K",
        "colab_type": "text"
      },
      "source": [
        "### Using HPC\n",
        "The following computations, training and evaluation was done on High Performance Cluster at Columbia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjdeedfWJOIM",
        "colab_type": "code",
        "outputId": "565ca126-bdb2-470f-a000-aaab2411c1be",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/rigel/home/sh3831\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgSjbQnxJcTj",
        "colab_type": "text"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q393uiGeup50",
        "outputId": "46bb83cd-a544-46b1-bbcb-45e666b2cac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from operator import itemgetter\n",
        "from rouge import Rouge \n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3atAH2GVJOIU",
        "colab_type": "code",
        "outputId": "e63cdae9-c6b7-4fb6-cbe4-5a7c270cabbc",
        "colab": {}
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60H5BsYxbjgq",
        "colab_type": "text"
      },
      "source": [
        "#### Maximum sequence length\n",
        "We experimented with different values for MAX_SEQ_LEN, starting with 20. We settled for 100, after trying out 20, 50, 100, 500 and 1000. For 500 and 1000 the training accuracy was higher for a single batch but overall Rouge score was lower. We got 96% accuracy for MAX_SEQ_LEN = 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OQTbTUlAVz0e",
        "colab": {}
      },
      "source": [
        "# SOS_TOKEN = '<sos>'\n",
        "# EOS_TOKEN = '<eos>'\n",
        "MAX_SEQ_LEN = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dIWzE5vq8Ue"
      },
      "source": [
        "## Load the train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjuGm3FTrFYK",
        "colab": {}
      },
      "source": [
        "train_embedding_path = '/rigel/home/sh3831/cnn-dailymail/sentence_embeddings/train/'\n",
        "valid_embedding_path = '/rigel/home/sh3831/cnn-dailymail/sentence_embeddings/valid/'\n",
        "train_label_path = '/rigel/home/sh3831/cnn-dailymail/labels/train/'\n",
        "valid_label_path = '/rigel/home/sh3831/cnn-dailymail/labels/valid/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1RAREQnlrbNU",
        "colab": {}
      },
      "source": [
        "# train_label_files and valid_label_files have the same name with the embedding files, so we don't need to read them again\n",
        "train_embedding_files = os.listdir(train_embedding_path)\n",
        "valid_embedding_files = os.listdir(valid_embedding_path)\n",
        "# train_label_files = os.listdir(train_label_path)\n",
        "# valid_label_files = os.listdir(valid_label_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zfup0zd6redH",
        "outputId": "57746f53-c5a2-443b-c703-14c9fd56055a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_embedding_files), len(valid_embedding_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23037, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ikdmVt-o1DM"
      },
      "source": [
        "## Create the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67T8WuRXKjqB",
        "colab_type": "text"
      },
      "source": [
        "Setting the basic variables for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "units = 1024\n",
        "EMBEDDING_SIZE = 200\n",
        "# vocab_inp_size = len(inp_lang.word_index)+1\n",
        "# vocab_tar_size = len(targ_lang.word_index)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihzdo4HNLJJb",
        "colab_type": "text"
      },
      "source": [
        "### Function to create dataset from given embeddings and labels\n",
        "We use it to create datasets for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eVs6ji5FtGLw",
        "colab": {}
      },
      "source": [
        "def create_dataset(embedding, label):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((embedding, label)).shuffle(BUFFER_SIZE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BekrpjYYM8at",
        "colab_type": "text"
      },
      "source": [
        "### Function to fetch embeddings and labels previously processed\n",
        "We use this function to fetch embeddings and labels for training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cMj9zWXsvMnx",
        "colab": {}
      },
      "source": [
        "def fetch_data(files, embedding_path, label_path):\n",
        "    label_list = []\n",
        "    embedding_list = []\n",
        "    for file_name in files:\n",
        "        embedding_temp = []\n",
        "        with open(embedding_path + file_name) as embed_json_file:\n",
        "            embed_data = json.load(embed_json_file)\n",
        "            for key, value in embed_data.items():\n",
        "                embedding_temp.append(value)\n",
        "        if len(embedding_temp) > MAX_SEQ_LEN:\n",
        "            padded_embed_value = embedding_temp[:MAX_SEQ_LEN].copy()\n",
        "        elif len(embedding_temp) < MAX_SEQ_LEN:\n",
        "            for i in range(MAX_SEQ_LEN- len(embedding_temp)):\n",
        "                embedding_temp.append(np.zeros(EMBEDDING_SIZE).tolist())\n",
        "            padded_embed_value = embedding_temp.copy()\n",
        "        else:\n",
        "            padded_embed_value = embedding_temp.copy()\n",
        "\n",
        "        embedding_list.append(padded_embed_value)\n",
        "        if label_path != '':\n",
        "            with open(label_path + file_name) as label_json_file:\n",
        "                label_data = json.load(label_json_file)\n",
        "                label_value = label_data['labels']\n",
        "            label_list.append(label_value)\n",
        "\n",
        "        if len(embedding_list) % 1000 == 0:\n",
        "            print(len(embedding_list))\n",
        "\n",
        "    return embedding_list, label_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4U_sb_KNLHR",
        "colab_type": "text"
      },
      "source": [
        "#### Fetching embeddings and labels for validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjMFUdyCJOIk",
        "colab_type": "code",
        "outputId": "e12d115f-b195-4388-e6a1-023d6c075675",
        "colab": {}
      },
      "source": [
        "embedding_valid, label_valid = fetch_data(valid_embedding_files, valid_embedding_path, valid_label_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-66xy0lJOIl",
        "colab_type": "code",
        "outputId": "ac5e5d99-116b-41ff-c998-6983c5bce532",
        "colab": {}
      },
      "source": [
        "len(embedding_valid), len(label_valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD5n91X1NXWG",
        "colab_type": "text"
      },
      "source": [
        "#### Converting validation embeddings and labels to tensors\n",
        "Also, labels need to be padded to be of equal length. In some cases labels are shorter because number of sentences in the article are less than MAX_SEQ_LEN, which we have set as 100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGPtWwX3JOIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_valid = tf.convert_to_tensor(embedding_valid)\n",
        "\n",
        "label_valid = tf.keras.preprocessing.sequence.pad_sequences(label_valid, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "label_valid = tf.convert_to_tensor(label_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuJGwWscJOIp",
        "colab_type": "code",
        "outputId": "16b85b20-b5b5-4753-e924-f90bc1f1909c",
        "colab": {}
      },
      "source": [
        "embedding_valid.shape, label_valid.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([5000, 20, 200]), TensorShape([5000, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3xbUUF7Pcfl",
        "colab_type": "text"
      },
      "source": [
        "#### Converting training embeddings and labels to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxhCP4HEJOIs",
        "colab_type": "code",
        "outputId": "fef1c6c2-5f14-4f72-8ab2-511d1b85fd1c",
        "colab": {}
      },
      "source": [
        "embedding_train, label_train = fetch_data(train_embedding_files, train_embedding_path, train_label_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8_lw0a-US_b",
        "colab_type": "text"
      },
      "source": [
        "Verifying the amount of data available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs5ZZyGpJOIt",
        "colab_type": "code",
        "outputId": "80bfba86-7b71-426e-cd4e-2dc1eaec0fe4",
        "colab": {}
      },
      "source": [
        "len(embedding_train), len(label_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23037, 23037)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJetL2JEUiqm",
        "colab_type": "text"
      },
      "source": [
        "Converting training embeddings and labels to tensors. As with validation data, we pad the labels here as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKFegufLJOIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_train = tf.convert_to_tensor(embedding_train)\n",
        "\n",
        "label_train = tf.keras.preprocessing.sequence.pad_sequences(label_train, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "label_train = tf.convert_to_tensor(label_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttSwqvi6JOIw",
        "colab_type": "code",
        "outputId": "6ef6d0f5-faa4-4b5a-e061-11fbc2456500",
        "colab": {}
      },
      "source": [
        "embedding_train.shape, label_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([23037, 20, 200]), TensorShape([23037, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wfN9EtYUzyU",
        "colab_type": "text"
      },
      "source": [
        "#### Creating the training and validation datasets using embeddings and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2RDUOKqDual6",
        "colab": {}
      },
      "source": [
        "train_ds = create_dataset(embedding_train, label_train)\n",
        "val_ds = create_dataset(embedding_valid, label_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONRww9LDVVb6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Creating an example batch of size 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "outputId": "239f585b-b21e-4421-cc6a-9bbe8b4afad5",
        "colab": {}
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_ds))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 20, 200]), TensorShape([64, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C4PHpbFBu8Yn",
        "outputId": "a47c32bb-7a00-4fa8-c542-dd2e52bee39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "example_input_batch, example_target_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=34, shape=(64, 20, 200), dtype=float32, numpy=\n",
              " array([[[ 1.19126707e-01,  1.02685206e-01, -6.63583651e-02, ...,\n",
              "          -1.64312925e-02, -2.11342499e-01,  2.94691250e-02],\n",
              "         [ 1.61275402e-01,  2.34291375e-01, -1.25948628e-02, ...,\n",
              "           1.11244068e-01, -2.03864664e-01,  8.24729800e-02],\n",
              "         [-4.03197929e-02,  1.13423169e-01, -9.32936594e-02, ...,\n",
              "           5.76375015e-02,  3.51726264e-02,  4.65227067e-02],\n",
              "         ...,\n",
              "         [-9.31013450e-02,  2.19360858e-01,  6.03526644e-02, ...,\n",
              "           8.99249613e-02, -1.92768145e-02,  7.37334043e-02],\n",
              "         [ 3.91648225e-02,  1.98101133e-01, -8.53032544e-02, ...,\n",
              "           3.90994214e-02, -7.28416303e-03,  4.79806252e-02],\n",
              "         [ 7.22724721e-02,  1.00568779e-01, -8.18326399e-02, ...,\n",
              "          -6.87590390e-02, -8.80567208e-02,  3.81180607e-02]],\n",
              " \n",
              "        [[ 1.55355260e-01,  3.39423180e-01,  2.19849162e-02, ...,\n",
              "          -1.18725665e-01, -7.68353343e-02, -1.00816665e-02],\n",
              "         [ 2.76925236e-01,  2.79814631e-01,  6.25915751e-02, ...,\n",
              "          -4.16708998e-02, -4.30748761e-02,  1.39274995e-03],\n",
              "         [ 4.29496169e-02,  3.37276846e-01, -2.57372916e-01, ...,\n",
              "          -3.46663855e-02, -5.12042604e-02, -9.76636186e-02],\n",
              "         ...,\n",
              "         [-2.16328502e-02,  4.07550007e-01,  9.46662482e-03, ...,\n",
              "           3.53244007e-01, -6.03282489e-02, -1.49387503e-02],\n",
              "         [ 5.72161339e-02,  1.50189072e-01, -9.60154459e-02, ...,\n",
              "          -7.60994665e-03, -1.34274632e-01, -5.63109480e-02],\n",
              "         [ 1.03180006e-03,  3.12009990e-01, -5.97679973e-01, ...,\n",
              "           1.44930005e-01,  5.25630005e-02,  7.50069976e-01]],\n",
              " \n",
              "        [[ 8.09517875e-02,  4.35966134e-01, -1.49891779e-01, ...,\n",
              "           4.47348431e-02, -1.87089935e-01, -1.24494426e-01],\n",
              "         [-2.44420879e-02,  1.12002499e-01, -2.03208223e-01, ...,\n",
              "          -3.78622636e-02, -1.74368754e-01,  9.74717587e-02],\n",
              "         [-3.68545540e-02,  2.06634745e-01, -3.73054028e-01, ...,\n",
              "           1.23492531e-01,  8.71425197e-02,  6.65032417e-02],\n",
              "         ...,\n",
              "         [-1.10500537e-01,  2.43125364e-01, -2.59690076e-01, ...,\n",
              "          -4.55926135e-02, -1.90784261e-01,  1.15928063e-02],\n",
              "         [-8.59512538e-02,  1.03258878e-01, -2.89136797e-01, ...,\n",
              "          -1.31593170e-02, -1.46990001e-01,  9.37166736e-02],\n",
              "         [-4.13965322e-02,  3.66946012e-01, -2.31087938e-01, ...,\n",
              "           2.05863342e-02,  8.84771645e-02, -6.46254569e-02]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-1.76774841e-02,  5.12356348e-02, -2.02829719e-01, ...,\n",
              "           4.09390852e-02, -1.49791107e-01, -1.05092004e-02],\n",
              "         [-1.74312502e-01,  2.15299994e-01,  7.16322511e-02, ...,\n",
              "          -1.73429996e-02, -1.84575006e-01,  3.78965735e-01],\n",
              "         [ 1.97127108e-02,  1.63058639e-01, -2.31389061e-01, ...,\n",
              "          -1.13139331e-01, -2.02667430e-01, -5.72493337e-02],\n",
              "         ...,\n",
              "         [ 5.71971685e-02,  7.47663304e-02, -3.44338655e-01, ...,\n",
              "          -6.86340034e-02,  1.98263340e-02, -1.04152836e-01],\n",
              "         [-1.61520854e-01, -1.49868220e-01, -2.50686228e-01, ...,\n",
              "           1.83725208e-01, -2.22275004e-01,  1.46478996e-01],\n",
              "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              " \n",
              "        [[-1.47064996e-03,  8.65663961e-02, -9.70102474e-02, ...,\n",
              "          -3.22839274e-04, -5.46017140e-02,  1.12124890e-01],\n",
              "         [-2.04541460e-01,  8.06252286e-02, -3.53071839e-01, ...,\n",
              "           7.50761554e-02,  8.30223858e-02, -6.23209998e-02],\n",
              "         [ 1.39272645e-01,  1.58701524e-01, -1.91207990e-01, ...,\n",
              "           1.62018538e-01,  2.65179239e-02,  2.57129222e-01],\n",
              "         ...,\n",
              "         [ 6.56952616e-03,  1.13034680e-01, -1.80022627e-01, ...,\n",
              "           3.65457907e-02, -1.11059844e-01, -2.54870523e-02],\n",
              "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              " \n",
              "        [[-1.40975714e-02,  7.05469996e-02,  2.80392859e-02, ...,\n",
              "           1.68477997e-01, -1.55010566e-01,  9.44971442e-02],\n",
              "         [ 1.03180006e-03,  3.12009990e-01, -5.97679973e-01, ...,\n",
              "           1.44930005e-01,  5.25630005e-02,  7.50069976e-01],\n",
              "         [ 8.67446661e-02,  1.17477052e-01, -1.42074049e-01, ...,\n",
              "           1.40952259e-01, -1.76802054e-01,  7.87528902e-02],\n",
              "         ...,\n",
              "         [ 7.16346428e-02,  2.29433417e-01, -2.37802595e-01, ...,\n",
              "           1.79902494e-01,  3.52488831e-02,  7.78835565e-02],\n",
              "         [-9.67523158e-02,  1.93017453e-01, -2.55894959e-01, ...,\n",
              "          -1.76595449e-02, -3.10254451e-02,  1.38066188e-01],\n",
              "         [-2.32913233e-02,  1.68807551e-01, -5.00764847e-02, ...,\n",
              "           1.33011550e-01, -5.21028861e-02,  9.51727778e-02]]],\n",
              "       dtype=float32)>, <tf.Tensor: id=35, shape=(64, 20), dtype=int32, numpy=\n",
              " array([[0, 1, 0, ..., 0, 0, 0],\n",
              "        [0, 1, 0, ..., 0, 0, 0],\n",
              "        [0, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 1, ..., 0, 0, 0],\n",
              "        [0, 1, 1, ..., 1, 0, 0],\n",
              "        [0, 1, 1, ..., 0, 0, 0]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MB0majtNXc09"
      },
      "source": [
        "## Define the bidirectional LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8vR7h15kXain",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten, Input, LSTM, Bidirectional, Activation, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kkPeml0xxHs",
        "colab_type": "text"
      },
      "source": [
        "We added Dropout to the model after we saw that the accuracy was hovering around 50%. Accuracy improved considerably after adding Dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FA_b4OX-oQ1E",
        "outputId": "74d87f35-bc7c-4549-89fd-56d1cad3ddce",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(MAX_SEQ_LEN, EMBEDDING_SIZE)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "# model.add(Bidirectional(LSTM(16)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(MAX_SEQ_LEN, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function standard_lstm at 0x2aab1d32dea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x2aab1d32dea0>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function cudnn_lstm at 0x2aab1d32df28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x2aab1d32df28>: AttributeError: module 'gast' has no attribute 'Num'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "scrolled": false,
        "outputId": "9338b828-73b7-474a-bb69-fd2e37fb7aea",
        "id": "gBge3qKDdK4a",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 20, 128)           135680    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 198,116\n",
            "Trainable params: 198,116\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ad271e0e-8ebf-4067-cfe5-3069f9eb1a46",
        "id": "riV7wCCC3oaC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAALlCAIAAABsMYcOAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE\nQVR4nOzdeVwTZ+I/8GdCLhII9yFySMCKFNRaD0StWldbX7ZWpCie1a27qLuLtB6sR1lLpZaiYmux\nVuu6Xd0ieNSDqnW9sIda24p4gaIVRdRwBwiSAPP7Y76bX5pwBEgIT/p5/8XMPPPM80ye+WRmMiQM\ny7IEAIA2PEs3AACgIxBeAEAlhBcAUAnhBQBU4lu6AUbZuHHj+fPnLd0KgN+FYcOGvf3225ZuRdvo\nCK/z589fuHAhLCzM0g0BsHIXLlywdBOMRUd4EULCwsL27t1r6VYAWLmoqChLN8FYuOcFAFRCeAEA\nlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEA\nlawnvAYPHmxjYzNgwICWChw9etTBweHIkSOGi+bPn29vb88wTE5OTpuFTcLc9a9fv97d3Z1hmK1b\ntxq/VlNTU2pqanh4uN78xMTE4OBgmUwmEokCAwOXL19eU1NjZJ3m7mkHXLhwoW/fvjwej2EYDw+P\ntWvXdtmm9+/fL5fLGYZhGMbT03PWrFldtmnrYz3hdenSpTFjxrRSoJUfefv888+3b99uZGGTMHf9\nS5cu/eGHH9q1yu3bt1944YW3335bpVLpLTp9+vRf//rXe/fulZaWvv/++5s2bTL+W5+64W/rhYWF\n3bx5c/z48YSQ/Pz81atXd9mmIyMj7969GxAQ4ODg8Pjx4927d3fZpq0PNV9GaCSGYVpaNHHixKqq\nKiPraVdhY9TV1Y0dO1YbKCavv5OuXLmSmJi4cOHC2tpaw7ixs7OLiYmxsbEhhEydOnX//v2ZmZkP\nHjzw8fFps+Yu66neHu4+um3DaGc9Z14cgUDQsRVbST2T2LFjh0KhMOsmOqN///779++fOXOmSCQy\nXJqVlcUlF8fV1ZUQYniCZlnddg9324bRztrCq6CgICgoSCqV2trajhw58rvvvuPmf/fdd76+vgzD\nfPLJJ9wclmVTUlL69OkjEokcHByWLVumrUSv8IcffiiRSOzt7RUKxZIlS3r27Jmfn9/Y2JiQkODr\n62tra9uvX7+MjAzt6rt27Ro0aJBYLJZKpb169Xrvvffi4uKWLFly584dhmECAwObbczGjRv79u0r\nEomcnJwmT56cl5fHLdqyZYtUKpVIJIcOHZowYYJMJvP29k5PT9du7ttvvw0ODnZwcBCLxaGhod98\n8405dzAhhDx8+NDW1tbf37/Nkno9bb0vH3/8sVgsdnd3X7BgQY8ePcRicXh4+MWLF7mlsbGxQqHQ\n09OTm/zLX/4ilUoZhiktLSWE6O1hQsjx48dlMllSUpIxPerKhhmj2dd0/vz53M2ygICAy5cvE0Lm\nzZsnkUgcHBwOHz5MCGl2TDY7eo1sRnfH0uD1119//fXX2yw2duxYuVz+66+/ajSaa9euDR06VCwW\n37p1i1v64MEDQsjmzZu5yVWrVjEMs2HDhoqKCpVKlZaWRgi5fPlyS4UJIYsXL968efOUKVNu3ry5\ndOlSkUi0b9++ioqKlStX8ni8S5cusSybmppKCFm3bl1ZWVl5eflnn302c+ZMlmUjIyMDAgK0TdWr\nPyEhQSgU7tq1q7KyMjc3d+DAga6uro8fP9bd+qlTp6qqqhQKxciRI6VSqVqt5pbu3bt3zZo15eXl\nZWVlYWFhLi4u3Pzbt28TQj799NN27eqhQ4f279+/lQK1tbX29vaxsbFGVtjsnmypLzExMVKp9MaN\nG0+fPr1+/frgwYPt7e3v37/PLZ05c6aHh4e25pSUFEJISUkJN6m3h7Oysuzt7RMTE1tq2EsvvUQI\nqaio6OKGsSzL3fNqZae19JpGRkba2Ng8fPhQW3LGjBmHDx/m/m5pTBqO3lY2beSx1h1YW3jpHni5\nubmEkKVLl3KTukeRSqWSSCTjxo3TFubeZlsPr7q6Om6yrq5OIpFER0dzkyqVSiQSLVq0SK1WOzo6\njhkzRlttQ0PDpk2b2FbDS6VS2dnZaWtjWfbHH38khGgPPL2tczlbUFBguAfef/99QohCoWDNFl6r\nVq165plnlEqlkRW2vif1+hITE6N7VF+6dIkQ8u6773KT7c2I1jUbXl3TsDbDS5fua3ry5ElCyNq1\na7lFVVVVvXv3bmhoYFsek4Zdax1F4WVtl426QkNDHRwcuAjTU1BQoFKpxo4d27Ga8/PzVSpVSEgI\nN2lra+vp6ZmXl5ebm1tZWckdFRwbG5vFixe3Xtv169dramoGDRqknTN48GChUKi9MNEjFAoJIRqN\nxnARd8uvsbGxnR0y1oEDBzIzM7/55ht7e3uTVNhKXwghgwYNkkgk2ivortR9Gqb7mr744ovPPPPM\nP//5T5ZlCSF79uyJjo7mbke2NCa7oIWWYs3hRQgRCATNjr+ioiJCiJubW8eqra2tJYSsXr2a+Z/C\nwkKVSqVUKgkhjo6O7aqtsrKSEGJnZ6c709HRsbq62pjVv/7669GjR7u5uYlEouXLl7dr0+2yZ8+e\nDz744OzZs7169TLfVvSIRKKSkpIu25zxzNqwll5ThmEWLFhw9+7dU6dOEUL+/e9/v/nmm9yilsak\nmVrYHVhzeDU0NJSXl/v6+houEovFhJD6+vqO1cylXmpqqu5J7Pnz5728vAgh3J1a43FhpxdVlZWV\n3t7eba57//79iIgIT0/PixcvVlVVJScnt2vTxtu8efPu3btPnz7N9bFraDQaI/dDFzNHw86dO8fd\nMG39NZ07d65YLP7888/z8/NlMpmfnx83v6UxacIWdjfWHF5nzpxpamoaOHCg4aKQkBAej5ednd2x\nmn18fMRisfZxfK1evXo5OzufOHGiXbWFhITY2dn99NNP2jkXL15Uq9XPP/98m+tevXpVo9EsWrRI\nLpeLxWJzPPDBsmx8fPzVq1cPHjyod3pobmfPnmVZVvtL6Xw+v6XruC5mjob9/PPPUqmUtPWaOjk5\nTZs27eDBg+vXr//Tn/6knd/SmLRi1hZearW6qqqqoaHhl19+iY2N9fPzmzt3rmExNze3yMjIffv2\n7dixQ6lU5ubmbtu2zfitiMXiefPmpaenb9myRalUNjY2FhUVPXr0SCQSrVy58ty5c7GxsQ8fPmxq\naqqurr5x4wYhxNnZubi4+N69e9XV1XoDXSwWL1my5MCBA7t371YqlVevXl24cGGPHj1iYmLabAl3\nXnny5MmnT5/evn27pdtknXHjxo0PP/xw+/btAoGA0bF+/XqTb4sQ0tTUVFFR0dDQkJubGxcX5+vr\nq30FAwMDy8vLDx48qNFoSkpKCgsLdVfU28PHjh0z/lGJrmyYYc0ajebJkydnz57lwqvN13ThwoX1\n9fVZWVmvvvqqdmZLY9JU3e+Ouu6zgU4w8hOQnTt3jhkzxt3dnc/nu7i4TJ8+vbCwkFu0efNm7kkc\niUQyadIklmWrq6vnz5/v4uJiZ2c3YsSIhIQEQoi3t/eVK1f0CicnJ9va2hJCfHx8du3axVVYX18f\nHx/v6+vL5/O5KLx+/Tq36JNPPgkNDRWLxWKx+LnnnktLS2NZ9pdffvHz87O1tR0xYsTq1av1GtPU\n1JSSktK7d2+BQODk5BQREZGfn8/VlpaWJpFICCG9e/e+c+fOtm3bZDIZIcTPz497CiQ+Pt7Z2dnR\n0TEqKop7nCogICAuLs7Dw4MQIpVKp0yZ0uauO3/+/PDhw3v06MGNCk9Pz/Dw8OzsbJZlr1692uzI\nSUlJabNavT3ZZl9iYmIEAkHPnj35fL5MJps8efKdO3e0tZWVlY0ZM0YsFvv7+//tb3/jHs0LDAzk\nHlnQ3cOPHz8+evSovb299oM5XRcuXHj22Wd5PB7X06SkpC5r2KeffhoQENDSwXjgwAGuwmZfU+2T\nGSzLPvfccytWrNDrV7NjstnR2wqKPm1k2O73r2eGuP+k27t3r6UbAua1YMGCvXv3lpWVWboh+rpb\nwyZOnPjJJ58Y85xwe1F0rFnbZSPQznzPeXSSxRumveTMzc3lzvIs2x6LQ3hZv7y8PKZl0dHR3bBm\nMBQfH3/79u1bt27Nmzfvvffes3RzLM/avlUCDAUFBZnp5oBpa165cuXOnTvVarW/v39KSsrrr79u\nqpo7qZs0TCKRBAUF9ezZMy0tLTg42CJt6FZwzwsA/j+KjjVcNgIAlRBeAEAlhBcAUAnhBQBUQngB\nAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlaj5SpwLFy5w/+8OAOZz4cIF\n7Q+LdHN0hNewYcMs3QRoN+73kHR/TBe6v7CwMFoONzq+zwtoNHXqVEJIZmampRsC1gn3vACASggv\nAKASwgsAqITwAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggv\nAKASwgsAqITwAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggv\nAKASwgsAqITwAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBKDMuylm4DWIl//etfmzZtamxs5CZL\nSkoIIW5ubtykjY1NXFzc3LlzLdU8sDIILzCZ/Pz8oKCgVgrcvHmz9QIAxsNlI5hMnz59QkNDGYYx\nXMQwTGhoKJILTAjhBaY0Z84cGxsbw/l8Pv+NN97o+vaAFcNlI5hScXGxt7e34aBiGOb+/fve3t4W\naRVYJZx5gSl5eXmFh4fzeL8ZVzweLzw8HMkFpoXwAhObPXu23m0vhmHmzJljqfaAtcJlI5hYeXm5\nh4dHQ0ODdo6Njc2TJ09cXFws2CqwPjjzAhNzdnYeN24cn8/nJm1sbMaNG4fkApNDeIHpzZo1q6mp\nifubZdnZs2dbtj1glXDZCKZXW1vr6ur69OlTQohIJCotLbWzs7N0o8Da4MwLTE8qlU6aNEkgEPD5\n/MmTJyO5wBwQXmAWM2fObGhoaGxsnDFjhqXbAtaJrztRVFT0ww8/WKopYE0aGxvFYjHLsjU1NZmZ\nmZZuDlgD/acFWR0ZGRmWaxgAQGsyMjJ084pvWAK38MEkzpw5wzDM6NGjLd0QsAaG//DfTHgBmMSo\nUaMs3QSwZggvMBe9/3AEMC0MLwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggv\nAKASwgsAqITwAgAqIbwAgErtDq/Bgwfb2NgMGDCgpQJHjx51cHA4cuSI4aL58+fb29szDJOTk9Nm\nYZMwd/3r1693d3dnGGbr1q3Gr9XU1JSamhoeHq43PzExMTg4WCaTiUSiwMDA5cuX19TUGK5+8uTJ\nFStWdGzTJteBvhw+fDg5ObmxsdHITezfv18ulzM6+Hy+q6vrH/7whwMHDuiWxNjjcCNEd795enrO\nmjWrpaquXLkSHR3t7+8vEolcXV379++/du1ablF0dDTTqqysLN0NvfPOO81uYuPGjQzD8Hi8oKCg\nc+fOtXcMNM/wywjZtowdO7Z///4tLc3KypLJZIcPH252aXp6OiHk8uXLxhTuPHPXz7Ls7du3CSGf\nfvqpkeVv3bo1fPhwQojhPhw1alRaWlpZWZlSqczIyBAIBC+//LJemYSEhFdffVWpVHZg0ybX4b5s\n2rRp1KhRFRUVxm8rICDAwcGB+7u8vPzkyZNBQUGEkD179mjLYOyxvx0h7G/3W7Nyc3MlEsnixYt/\n/fXXurq6/Pz85cuXjx07lls6bdq0EydOVFZWajSaR48eEUImTZqkVqtra2sVCsWf/vSnI0eOaDdE\nCPH09FSr1XqbaGho8PPzI4Roq2XbPwaIwZcRdjC8BgwYYOQm9egNIJNTqVTDhg0zU+XNaleC5OTk\nTJkyZffu3QMGDDA84CdOnNjQ0KCdnDp1KiHk/v372jnr1q175pln6urqjN+0+XZIJ/sSGxs7bNgw\njUZj5OYMD8JvvvmGEDJlyhQja/g9jD29EcIaEV5z5szx8vLSnVNfX//KK69wf0dHR9fW1nJ/c+H1\n2muvaUtu3bpVN7yef/55QkhmZqbeJjIyMrhzc93wYts5BgzDq4P3vAQCQcdWNPw6RNPasWOHQqEw\n6yY6o3///vv37585c6ZIJDJcmpWVZWNjo510dXUlhKhUKm6yoKDgnXfeeffdd8VisfFbNN8O6Uxf\nCCFr1qzJycnZtGlThxvQq1cvQkhlZaWR5a1+7HVshJSVlVVVVZWXl2vnCIVC7dVuenq6RCJpad2Y\nmJhXXnlFO7lo0SJCyKeffqpXbOPGjUuWLDFcvZNjoIPhVVBQEBQUJJVKbW1tR44c+d1333Hzv/vu\nO19fX4ZhPvnkE24Oy7IpKSl9+vQRiUQODg7Lli3TVqJX+MMPP5RIJPb29gqFYsmSJT179szPz29s\nbExISPD19bW1te3Xr5/ut+zv2rVr0KBBYrFYKpX26tXrvffei4uLW7JkyZ07dxiGCQwMbLYxGzdu\n7Nu3r0gkcnJymjx5cl5eHrdoy5YtUqlUIpEcOnRowoQJMpnM29ube6/mfPvtt8HBwQ4ODmKxODQ0\nlHvbN6uHDx/a2tr6+/tzkx9//DHLspMmTWqpfHZ29pAhQyQSiUwmCw0NVSqVejtk06ZNUqmUx+M9\n//zzHh4eAoFAKpUOHDhw5MiRPj4+YrHY0dFx+fLlXdAXQoiTk9OoUaM2bdrEvakeP35cJpMlJSUZ\nX2dubi7R+b5WjL02R0izBg8eXFtb++KLL37//fftWtHQiy++2Ldv3zNnzuTn52tnfv/99yqVavz4\n8Ybl9cZAu+md3RHjLhvlcvmvv/6q0WiuXbs2dOhQsVh869YtbumDBw8IIZs3b+YmV61axTDMhg0b\nKioqVCpVWloa0Tl1NyxMCFm8ePHmzZunTJly8+bNpUuXikSiffv2VVRUrFy5ksfjXbp0iWXZ1NRU\nQsi6devKysrKy8s/++yzmTNnsiwbGRkZEBCgbape/QkJCUKhcNeuXZWVlbm5uQMHDnR1dX38+LHu\n1k+dOlVVVaVQKEaOHCmVSrUX8Hv37l2zZk15eXlZWVlYWJiLiws3v2M3noYOHdrKfUOWZWtra+3t\n7WNjY7Vz5HJ5cHCwbhndTdfU1MhksuTk5Lq6usePH0+ZMqWkpMRwh/zjH/8ghFy8eLG2tra0tPTl\nl18mhHz99dclJSW1tbWxsbGEkJycHHP3hbNixQrtYMjKyrK3t09MTGypEt3LH5VKdezYMT8/v/Hj\nx9fU1GjL/M7HnuEIYY24bFSpVIMGDeLSIDg4ODk5uaysrNmShpeNehv69ddfP/roI0JIXFycdn5E\nRMTOnTurq6uJwWUj+9sx0DpiqnteuoOVewNcunQpN6n7mqlUKolEMm7cOG1hvfsOzQ4g7RV7XV2d\nRCKJjo7mJlUqlUgkWrRokVqtdnR0HDNmjLbahoYGLr9bGUAqlcrOzk5bG8uyP/74IyFEe8DobZ0b\n6wUFBYZ74P333yeEKBQK1mzhtWrVqmeeeUZ727WmpoZhmFdffVW3jO6mr127RgjJysrSq6fZ8Kqu\nruYmv/jiC0LI1atXuUluh+jeAjdHX7T++c9/EkL+/e9/G7MV7n6wrtDQ0C+++KK+vl5b5vc89pod\nIawR4cWyrFqt/uijj7gPQAgh7u7uZ8+eNSxmTHhVVlZKpVInJyeVSsWy7J07d7y9vevr61sKL+PH\ngGF4meA5r9DQUAcHBy7C9BQUFKhUqrFjx3as5vz8fJVKFRISwk3a2tp6enrm5eXl5uZWVla+9NJL\n2pI2NjaLFy9uvbbr16/X1NRo32QIIYMHDxYKhRcvXmy2vFAoJIRoNBrDRdwtv85+0NuyAwcOZGZm\nfvPNN/b29twcbrC2cvdBLpe7u7vPmjVrzZo19+7dM3JDXB8bGhq4Sa5fzXa5wwz7osV158mTJ0ZW\npT0INRpNUVHRW2+9FRsb269fv9LSUsPCv7ex1+YIaYVAIIiNjb158+aFCxcmT56sUCiioqIqKio6\nUJWDg8OMGTMqKir27NlDCElNTV20aBHXnWa1dwzoMs1DqgKBoNkdXVRURAhxc3PrWLW1tbWEkNWr\nV2sfKiksLFSpVEqlkhDi6OjYrtq4O7t6Pz3v6OjIvSe06euvvx49erSbm5tIJDLTjSHOnj17Pvjg\ng7Nnz3I3pDlPnz4lhDR7a5xja2t7+vTpESNGJCUlyeXy6Ojouro68zXSSM32RcvW1pb8r2vtwufz\ne/bsOW/evPXr1+fn569bt86wzO9t7LU5QowxdOjQr776auHChSUlJWfOnOlYJdxt+61bt1ZWVu7d\nu3fBggWtFO7wGCAmCa+Ghoby8nJfX1/DRdynHvX19R2rmRt5qampuueK58+f9/LyIoQ0+37bCm7A\n6Q2XysrK3/wGbwvu378fERHh6el58eLFqqqq5OTkdm3aeJs3b969e/fp06e5Pmpxr3Hr53rPPvvs\nkSNHiouL4+PjMzIy1q9fb6ZGGqmlvmip1Wryv651TGhoKCHkxo0bhot+b2PPmBGide7cOe7OHSEk\nMjJSe+rNmT17NvntR8PtMmDAgLCwsB9//DEmJiYqKsrJyamVwp0ZAyYIrzNnzjQ1NQ0cONBwUUhI\nCI/Hy87O7ljN3Edg2keitXr16uXs7HzixIl21RYSEmJnZ/fTTz9p51y8eFGtVnMPp7Tu6tWrGo1m\n0aJFcrlcLBab40N3lmXj4+OvXr168OBBvbdoQgj3LHVVVVVLqxcXF3PHsJub27p16wYOHNjsId01\nWu+LFtcdDw+PDm/o559/JoT06dPHcNHvbey1OUJ0/fzzz1KplPu7vr5eb6hwnxX269fPmKqaxZ18\n7du376233mq9ZGfGQAfDS61WV1VVNTQ0/PLLL7GxsX5+fnPnzjUs5ubmFhkZuW/fvh07diiVytzc\n3G3bthm/FbFYPG/evPT09C1btiiVysbGxqKiokePHolEopUrV547dy42Nvbhw4dNTU3V1dXcC+Ds\n7FxcXHzv3r3q6mq9K1mxWLxkyZIDBw7s3r1bqVRevXp14cKFPXr0iImJabMl3HnlyZMnnz59evv2\n7ZZuVXTGjRs3Pvzww+3btwsEAt1/v+BOoCQSiVwu5y6FmlVcXLxgwYK8vDy1Wn358uXCwsKwsDDS\n6g4xn9b7osV1hzt7OnbsmDGPStTV1TU1NbEsW1xcvHPnztWrV7u6ujZ7hPzexl6bI4Sj0WiePHly\n9uxZbXgRQiIiIjIzMysrK6uqqg4dOvT3v//9tdde60x4TZ061dXVNSIiQi6Xt15Sdwy0m+5ZsZGf\nNu7cuXPMmDHu7u58Pt/FxWX69OmFhYXcos2bN3t6ehJCJBLJpEmTWJatrq6eP3++i4uLnZ3diBEj\nEhISCCHe3t5XrlzRK5ycnMydPfr4+OzatYursL6+Pj4+3tfXl8/nc8Px+vXr3KJPPvkkNDRULBaL\nxeLnnnsuLS2NZdlffvnFz8/P1tZ2xIgRq1ev1mtMU1NTSkpK7969BQKBk5NTREREfn4+V1taWhp3\n77B379537tzZtm2bTCYjhPj5+XFPgcTHxzs7Ozs6OkZFRXEP7wQEBMTFxXFvGlKp1JhHvc+fPz98\n+PAePXpwO9/T0zM8PDw7O5tl2atXrzb7AqWkpHDrxsbGCgQC7kMclmU3bNigu+l79+6Fh4c7OTnZ\n2Nh4eXmtWrWKe8Bdd4esWLGC62OvXr2+/fbbDz74wMHBgRDi4eHxn//8Z8+ePVyFTk5O6enpZu0L\nZ+LEiT179uTC6OjRo/b29mvXrjXc0IEDBww/ahSJRL179160aJH2qX2MPb0R0ux+0zpw4ABX7MSJ\nE9OmTQsICBCJREKhsE+fPmvWrHn69KnuS6BUKl944QVnZ2dCCI/HCwwMTEpKMnyBXF1d//rXv3Iz\nly9f/sMPP3B/a/cGj8cLDg7+9ttvmx0DrSMmeVQCLOL27dt8Pl97aNGutLRULBavX7/e0g2xHtSN\nkHaNAcPwwlfiUCMwMDAxMTExMbHZr5qgzpo1awYMGMA9FgsmQd0I6eQYQHiZTF5eXivfHBIdHd35\nTaxYsSIqKio6OtrI+7IdZu6+bNy4MScn5+jRox3+J1loVpeNkM7r/Bjgm7ZBv2dBQUFsx/5Fqz2S\nkpJOnDixbt26Dz74wHxbMWtfDh06VF9ff/bsWd3/3AZT6ZoR0kkmGQOM7hjNzMycNm1aFxyBAADt\nwjBMRkYG991KHFw2AgCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBA\nJYQXAFAJ4QUAVEJ4AQCVmvlKnMzMzK5vBwBAuzQTXtOmTev6dgAAtAuDb+8CM+G+egkn8mAmuOcF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVOJbugFgPbKzsy9cuKCdzMvL\nI4QkJydr54SFhY0aNcoCLQNrxLAsa+k2gJX473//O378eIFAwOPpn9E3NTVpNJoTJ06MGzfOIm0D\n64PwApNpbGz08PAoKytrdqmTk5NCoeDzcbIPpoF7XmAyNjY2M2fOFAqFhouEQuHs2bORXGBCCC8w\npenTp6vVasP5arV6+vTpXd8esGK4bAQT8/Pzu3//vt5Mb2/v+/fvMwxjkSaBVcKZF5jYrFmzBAKB\n7hyhUPjGG28gucC0cOYFJnbz5s3g4GC9mVevXg0JCbFIe8BaIbzA9IKDg2/evKmdDAoK0p0EMAlc\nNoLpzZkzR3vlKBAI3njjDcu2B6wSzrzA9O7fv9+rVy9uaDEMc/fu3V69eisn/BMAACAASURBVFm6\nUWBtcOYFpufr6zto0CAej8cwzODBg5FcYA4ILzCLOXPm8Hg8Gxub2bNnW7otYJ1w2QhmUVJS0qNH\nD0LIw4cPPTw8LN0csEasjoyMDEs3BwCgeRkZGbp51cz/miHCwCSys7MZhnnhhRcs3RCwBtOmTdOb\n00x4TZ06tUsaA1bu5ZdfJoTIZDJLNwSsgVHhBWASiC0wK3zaCABUQngBAJUQXgBAJYQXAFAJ4QUA\nVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUand4DR482MbGZsCAAS0VOHr0\nqIODw5EjRwwXzZ8/397enmGYnJycNgubhLnrX79+vbu7O8MwW7duNX6tpqam1NTU8PBwvfnJyclB\nQUG2trZSqTQoKOidd95RKpWGq588eXLFihUd27TJdaAvhw8fTk5ObmxsNHIT+/fvl8vljA4+n+/q\n6vqHP/zhwIEDuiUx9jjcCNHdb56enrNmzWqpqitXrkRHR/v7+4tEIldX1/79+69du5ZbFB0dzbQq\nKytLd0PvvPNOs5vYuHEjwzA8Hi8oKOjcuXPtHQPNM/wmVbYtY8eO7d+/f0tLs7KyZDLZ4cOHm12a\nnp5OCLl8+bIxhTvP3PWzLHv79m1CyKeffmpk+Vu3bg0fPpwQYrgPJ06cuH79eoVCUV1dnZmZKRAI\nxo0bp1cmISHh1VdfVSqVHdi0yXW4L5s2bRo1alRFRYXx2woICHBwcOD+Li8vP3nyZFBQECFkz549\n2jIYe+xvRwj72/3WrNzcXIlEsnjx4l9//bWuri4/P3/58uVjx47llk6bNu3EiROVlZUajebRo0eE\nkEmTJqnV6traWoVC8ac//enIkSPaDRFCPD091Wq13iYaGhr8/PwIIdpq2faPAWLwTaodDK8BAwYY\nuUk9egPI5FQq1bBhw8xUebPalSA5OTlTpkzZvXv3gAEDDA/4iIiIuro67WRUVBQhpLi4WDtn3bp1\nzzzzjLaMMZs23w7pZF9iY2OHDRum0WiM3JzhQfjNN98QQqZMmWJkDb+Hsac3QlgjwmvOnDleXl66\nc+rr61955RXu7+jo6NraWu5vLrxee+01bcmtW7fqhtfzzz9PCMnMzNTbREZGBndurhtebDvHgGF4\ndfCel/YnRduLYZiOrWikHTt2KBQKs26iM/r3779///6ZM2eKRCLDpQcOHBCLxdrJnj17EkJqamq4\nyYKCgnfeeefdd9/VLdMm8+2QzvSFELJmzZqcnJxNmzZ1uAHcL6pVVlYaWd7qx17HRkhZWVlVVVV5\nebl2jlAo1F7tpqenSySSltaNiYl55ZVXtJOLFi0ihHz66ad6xTZu3LhkyRLD1Ts5BjoYXgUFBUFB\nQVKp1NbWduTIkd999x03/7vvvvP19WUY5pNPPuHmsCybkpLSp08fkUjk4OCwbNkybSV6hT/88EOJ\nRGJvb69QKJYsWdKzZ8/8/PzGxsaEhARfX19bW9t+/frpfr/+rl27Bg0aJBaLpVJpr1693nvvvbi4\nuCVLlty5c4dhmMDAwGYbs3Hjxr59+4pEIicnp8mTJ+fl5XGLtmzZIpVKJRLJoUOHJkyYIJPJvL29\nufdqzrfffhscHOzg4CAWi0NDQ7m3fbO6ffu2o6Mjd75NCPn4449Zlp00aVJL5bOzs4cMGSKRSGQy\nWWhoqFKp1NshmzZtkkqlPB7v+eef9/DwEAgEUql04MCBI0eO9PHxEYvFjo6Oy5cv74K+EEKcnJxG\njRq1adMm7k31+PHjMpksKSnJ+Dpzc3MJIaNGjeImMfbaHCHNGjx4cG1t7Ysvvvj999+3a0VDL774\nYt++fc+cOZOfn6+d+f3336tUqvHjxxuW1xsD7aZ3dkeMu2yUy+W//vqrRqO5du3a0KFDxWLxrVu3\nuKUPHjwghGzevJmbXLVqFcMwGzZsqKioUKlUaWlpROfU3bAwIWTx4sWbN2+eMmXKzZs3ly5dKhKJ\n9u3bV1FRsXLlSh6Pd+nSJZZlU1NTCSHr1q0rKysrLy//7LPPZs6cybJsZGRkQECAtql69SckJAiF\nwl27dlVWVubm5g4cONDV1fXx48e6Wz916lRVVZVCoRg5cqRUKtVewO/du3fNmjXl5eVlZWVhYWEu\nLi7c/I7deBo6dGhL9w3VanVRUdHmzZtFItGuXbu08+VyeXBwsG5J3U3X1NTIZLLk5OS6urrHjx9P\nmTKlpKTEcIf84x//IIRcvHixtra2tLSU+5r5r7/+uqSkpLa2NjY2lhCSk5Nj7r5wVqxYoR0MWVlZ\n9vb2iYmJLW1F9/JHpVIdO3bMz89v/PjxNTU12jK/87FnOEJYIy4bVSrVoEGDuDQIDg5OTk4uKytr\ntqThZaPehn799dePPvqIEBIXF6edHxERsXPnzurqamJw2cj+dgy0jpjqnpfuYOXeAJcuXcpN6r5m\nKpVKIpHo3qnVu+/Q7ADSXrHX1dVJJJLo6GhuUqVSiUSiRYsWqdVqR0fHMWPGaKttaGjg8ruVAaRS\nqezs7LS1sSz7448/EkK0B4ze1rmxXlBQYLgH3n//fUKIQqFgzRBe3K8curi4fPTRR9rhW1NTwzDM\nq6++qltSd9PXrl0jhGRlZenV1mx4VVdXc5NffPEFIeTq1avcJLdDdG+Bm6MvWv/85z8JIf/+97+N\n2Qp3P1hXaGjoF198UV9fry3zex57zY4Q1ojwYllWrVZ/9NFH3AcghBB3d/ezZ88aFjMmvCorK6VS\nqZOTk0qlYln2zp073t7e9fX1LYWX8WPAMLxM8JxXaGiog4MDF2F6CgoKVCrV2LFjO1Zzfn6+SqUK\nCQnhJm1tbT09PfPy8nJzcysrK1966SVtSRsbm8WLF7de2/Xr12tqarRvMoSQwYMHC4XCixcvNlte\nKBQSQjQajeEi7pZfZz/obcGDBw8UCsWXX375xRdfPPfcc9xtFG6wtnL3QS6Xu7u7z5o1a82aNffu\n3TNyW1wfGxoauEmuX812uWOa7YsW150nT54YWZv2INRoNEVFRW+99VZsbGy/fv1KS0sNC//exl6b\nI6QVAoEgNjb25s2bFy5cmDx5skKhiIqKqqio6EBVDg4OM2bMqKio2LNnDyEkNTV10aJFXHea1d4x\noMs0D6kKBIJmd3RRUREhxM3NrWPV1tbWEkJWr16tfaiksLBQpVJxTww5Ojq2qzbuzq6dnZ3uTEdH\nR+49oU1ff/316NGj3dzcRCKRmW4McQQCgZub2/jx4/fs2XP9+nXunfbp06eEkGZvjXNsbW1Pnz49\nYsSIpKQkuVweHR1dV1dnvkYaqdm+aNna2pL/da1d+Hx+z549582bt379+vz8/HXr1hmW+b2NvTZH\niDGGDh361VdfLVy4sKSk5MyZMx2rhLttv3Xr1srKyr179y5YsKCVwh0eA8Qk4dXQ0FBeXu7r62u4\niPvUo76+vmM1cyMvNTVV91zx/PnzXl5ehJBm329bwQ04veFSWVnp7e3d5rr379+PiIjw9PS8ePFi\nVVVVcnJyuzbdMYGBgTY2NtevXyf/e41bP9d79tlnjxw5UlxcHB8fn5GRsX79+i5opJF0+6KlVqvJ\n/7rWMaGhoYSQGzduGC76vY09Y0aI1rlz57g7d4SQyMhI7ak3Z/bs2YQQlUplTFWGBgwYEBYW9uOP\nP8bExERFRTk5ObVSuDNjwAThdebMmaampoEDBxouCgkJ4fF42dnZHauZ+whM+0i0Vq9evZydnU+c\nONGu2kJCQuzs7H766SftnIsXL6rVau7hlNZdvXpVo9EsWrRILpeLxWJzfOheVlY2Y8YM3Tm3b99u\nbGz08fEhhHDPUldVVbW0enFxMXcMu7m5rVu3buDAgc0e0l2j9b5ocd3h7ot1zM8//0wI6dOnj+Gi\n39vYa3OE6Pr555+lUin3d319vd5Q4T4r7NevnzFVNYs7+dq3b99bb73VesnOjIEOhpdara6qqmpo\naPjll19iY2P9/Pzmzp1rWMzNzS0yMnLfvn07duxQKpW5ubnbtm0zfitisXjevHnp6elbtmxRKpWN\njY1FRUWPHj0SiUQrV648d+5cbGzsw4cPm5qaqquruRfA2dm5uLj43r171dXVeleyYrF4yZIlBw4c\n2L17t1KpvHr16sKFC3v06BETE9NmS7jzypMnTz59+vT27dst3aroDKlUeuLEidOnTyuVSo1Gc/ny\n5TfeeEMqlb799tuEEIlEIpfLuUuhZhUXFy9YsCAvL0+tVl++fLmwsDAsLIy0ukPMp/W+aHHd4c6e\njh07ZsyjEnV1dU1NTSzLFhcX79y5c/Xq1a6urs0eIb+3sdfmCOFoNJonT56cPXtWG16EkIiIiMzM\nzMrKyqqqqkOHDv39739/7bXXOhNeU6dOdXV1jYiIkMvlrZfUHQPtpntWbOSnjTt37hwzZoy7uzuf\nz3dxcZk+fXphYSG3aPPmzZ6enoQQiUQyadIklmWrq6vnz5/v4uJiZ2c3YsSIhIQEQoi3t/eVK1f0\nCicnJ3Nnjz4+PtqP1evr6+Pj4319ffl8Pjccr1+/zi365JNPQkNDxWKxWCx+7rnn0tLSWJb95Zdf\n/Pz8bG1tR4wYsXr1ar3GNDU1paSk9O7dWyAQODk5RURE5Ofnc7WlpaVx9w579+59586dbdu2cT/4\n7Ofnxz0FEh8f7+zs7OjoGBUVxT28ExAQEBcXx71pSKVSYx71Pn/+/PDhw3v06MHtfE9Pz/Dw8Ozs\nbG7ppEmT/P397ezsRCJRQEBAdHS09nNAlmVjY2MFAgH3IQ7Lshs2bNDd9L1798LDw52cnGxsbLy8\nvFatWtXQ0KC3Q1asWMH1sVevXt9+++0HH3zg4OBACPHw8PjPf/6zZ88erkInJ6f09HSz9oUzceLE\nnj17cmF09OhRe3v7tWvXGm7owIEDhh81ikSi3r17L1q06P79+1wxjD29EdLsftM6cOAAV+zEiRPT\npk0LCAgQiURCobBPnz5r1qx5+vSp7kugVCpfeOEFZ2dnQgiPxwsMDExKSjJ8gVxdXf/6179yM5cv\nX/7DDz9wf2v3Bo/HCw4O/vbbb5sdA60jJnlUAizi9u3bfD7f8GkpSpWWlorF4vXr11u6IdaDuhHS\nrjFgGF74ShxqBAYGJiYmJiYm6v6TDb3WrFkzYMAA7rFYMAnqRkgnxwDCy2Ty8vJa+eaQ6Ojozm9i\nxYoVUVFR0dHRRt6X7TBz92Xjxo05OTlHjx7t8D/JQrO6bIR0XufHAN+0Dfo9CwoKYjv2L1rtkZSU\ndOLEiXXr1n3wwQfm24pZ+3Lo0KH6+vqzZ8/a2NiYaRO/Z10zQjrJJGOA0R2jmZmZ06ZN64IjEACg\nXRiGycjImDp1qnYOLhsBgEoILwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggv\nAKASwgsAqITwAgAqIbwAgErNfCWOOX5dAgDAtH7zlThFRUU//PCDBVsD1oT7ca02fz8GwEjh4eG6\nPxbH4Nu7wEy4r17KzMy0dEPAOuGeFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBA\nJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBA\nJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBA\nJb6lGwDWo7S0VKlUaidra2sJIXfv3tXOkclkrq6uFmgZWCOGZVlLtwGsxI4dO+bPn99Kgc8///zN\nN9/ssvaAdUN4gclUVFR4eHhoNJpmlwoEgidPnjg5OXVxq8Ba4Z4XmIyTk9PLL7/M5zdzL4LP50+Y\nMAHJBSaE8AJTmjVrVmNjo+H8xsbGWbNmdX17wIrhshFM6enTpy4uLiqVSm++ra1taWmpRCKxSKvA\nKuHMC0xJLBZHREQIBALdmQKBIDIyEskFpoXwAhObMWOG3j17jUYzY8YMS7UHrBUuG8HEGhoa3N3d\nKyoqtHMcHR0VCoXe6RhAJ+HMC0yMz+dHR0cLhUJuUiAQzJgxA8kFJofwAtObPn26Wq3m/tZoNNOn\nT7dse8Aq4bIRTI9lWW9v7+LiYkKIp6dncXExwzCWbhRYG5x5gekxDDNr1iyhUCgQCObMmYPkAnNA\neIFZcFeO+JwRzAffKtEtREVFWboJpmdnZ0cIWbt2raUbYnp79+61dBMA97y6B4ZhwsLCvL29Ld0Q\nU7p58yYhpG/fvpZuiCkVFRVduHABR013gPDqFhiGycjImDp1qqUbYkp37twhhAQEBFi6IaaUmZk5\nbdo0HDXdAS4bwVysLLagu8ENewCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggv\nAKASwgsAqITwAgAqIbwAgEoILwCgEsKLSvPnz7e3t2cYJicnx9Jt+f++/PLLwYMH29vb+/n5zZs3\n7/Hjx8astX//frlczugQCoXu7u6jR49OSUnR/Qk1AF0ILyp9/vnn27dvt3QrfiMjI2PmzJlRUVFF\nRUWHDh06d+7chAkTGhoa2lwxMjLy7t27AQEBDg4OLMs2NTUpFIrMzEx/f//4+Phnn332p59+6oL2\nA3UQXmAan332mZeX17JlyxwcHAYMGPD222/n5ORcvHixvfUwDOPo6Dh69OidO3dmZmY+efJk4sSJ\nVVVV5mgzUA3hRavu9pM8Dx486NGjh7ZVPj4+hJDCwsLO1Pn666/PnTtXoVBs3brVBE0E64LwogbL\nsikpKX369BGJRA4ODsuWLdNd2tjYmJCQ4Ovra2tr269fv4yMDELIli1bpFKpRCI5dOjQhAkTZDKZ\nt7d3enq6dq3s7OwhQ4ZIJBKZTBYaGqpUKluqqk1yuVyhUGgnuRtecrmcmzx+/LhMJktKSmpvr+fO\nnUsIOXbsWDfpJnQjLHQDhJCMjIzWy6xatYphmA0bNlRUVKhUqrS0NELI5cuXuaVLly4ViUT79u2r\nqKhYuXIlj8e7dOkStxYh5NSpU1VVVQqFYuTIkVKpVK1WsyxbU1Mjk8mSk5Pr6uoeP348ZcqUkpKS\nVqpq3dmzZwUCwccff6xUKq9du9a3b9+XXnpJuzQrK8ve3j4xMbGl1bX3vPRwQePj49NNusllXJvF\noAvgZegW2gwvlUolkUjGjRunncOdWXDhVVdXJ5FIoqOjtYVFItGiRYvY/x3VdXV13CIu8goKCliW\nvXbtGiEkKytLd0OtVNWm1atXa98Uvb29Hzx4YGT32ZbDi2VZ7i5YN+kmwqv7wGUjHQoKClQq1dix\nY5tdmp+fr1KpQkJCuElbW1tPT8+8vDzDkkKhkBCi0WgIIXK53N3dfdasWWvWrLl37157q9KzatWq\nbdu2nTp1qqam5u7du+Hh4cOGDXvw4EG7u/pbtbW1LMvKZLJu0k3oPhBedCgqKiKEuLm5Nbu0traW\nELJ69Wrto1KFhYUqlar1Om1tbU+fPj1ixIikpCS5XB4dHV1XV9exqh49epScnPznP//5xRdflEql\n/v7+27dvLy4uTklJ6Uhvddy6dYsQEhQU1B26Cd0KwosOYrGYEFJfX9/sUi7UUlNTdU+qz58/32a1\nzz777JEjR4qLi+Pj4zMyMtavX9+xqm7fvt3Y2Ojl5aWdI5PJnJ2dr1+/bnwfm3X8+HFCyIQJE0g3\n6CZ0KwgvOoSEhPB4vOzs7GaX+vj4iMXi9j5tX1xcfOPGDUKIm5vbunXrBg4ceOPGjY5Vxf3W96NH\nj7Rzqqury8vLuQcmOuzx48epqane3t5//OMfSTfoJnQrCC86uLm5RUZG7tu3b8eOHUqlMjc3d9u2\nbdqlYrF43rx56enpW7ZsUSqVjY2NRUVFulHSrOLi4gULFuTl5anV6suXLxcWFoaFhXWsKn9//zFj\nxmzfvv3cuXN1dXUPHjyIiYkhhLz55ptcgWPHjrX5qATLsjU1NU1NTSzLlpSUZGRkDB8+3MbG5uDB\ng9w9L4t3E7oXM30QAO1CjHhUorq6ev78+S4uLnZ2diNGjEhISCCEeHt7X7lyhWXZ+vr6+Ph4X19f\nPp/PJd3169fT0tIkEgkhpHfv3nfu3Nm2bRuXAn5+frdu3bp37154eLiTk5ONjY2Xl9eqVasaGhpa\nqqrNLpSWlsbFxQUGBopEIjs7u+HDh3/11VfapUePHrW3t1+7dq3hiocPH+7Xr59EIhEKhTwej/zv\nIfshQ4YkJiaWlZXpFrZ4N/FpY/fBsCxrueSE/8MwTEZGxtSpUy3dEGhDZmbmtGnTcNR0B7hsBAAq\nIbygbXl5eUzLoqOjLd1A+D3iW7oBQIGgoCBcKEF3gzMvAKASwgsAqITwAgAqIbwAgEoILwCgEsIL\nAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggvAKASwgsAqIRvUu0WGIYJCwvjfsYCurOi\noqILFy7gqOkOEF7dQlRUlKWbYHo//fQTIWTQoEGWbojp7d2719JNAIQXmA33lfyZmZmWbghYJ9zz\nAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggvAKASwgsAqITw\nAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACohvACASggvAKASwgsAqITw\nAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBKCC8AoBLCCwCohPACACoxLMtaug1gJf71r39t2rSp\nsbGRmywpKSGEuLm5cZM2NjZxcXFz5861VPPAyiC8wGTy8/ODgoJaKXDz5s3WCwAYD5eNYDJ9+vQJ\nDQ1lGMZwEcMwoaGhSC4wIYQXmNKcOXNsbGwM5/P5/DfeeKPr2wNWDJeNYErFxcXe3t6Gg4phmPv3\n73t7e1ukVWCVcOYFpuTl5RUeHs7j/WZc8Xi88PBwJBeYFsILTGz27Nl6t70YhpkzZ46l2gPWCpeN\nYGLl5eUeHh4NDQ3aOTY2Nk+ePHFxcbFgq8D64MwLTMzZ2XncuHF8Pp+btLGxGTduHJILTA7hBaY3\na9aspqYm7m+WZWfPnm3Z9oBVwmUjmF5tba2rq+vTp08JISKRqLS01M7OztKNAmuDMy8wPalUOmnS\nJIFAwOfzJ0+ejOQCc0B4gVnMnDmzoaGhsbFxxowZlm4LWCe+pRtgbTIzMy3dhG6hsbFRLBazLFtT\nU4N9wpk6daqlm2BVcM/LxJr9zz4AQgiONdPCmZfpZWRk4D2WEHLmzBmGYUaPHm3phlheZmbmtGnT\nLN0Ka4PwAnMZNWqUpZsA1gzhBeai9x+OAKaF4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBe\nAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4WVh8+fPt7e3ZxgmJyfH0m3prC+//HLw4MH29vZ+\nfn7z5s17/PixMWvt379fLpczOoRCobu7++jRo1NSUioqKszdbKAUwsvCPv/88+3bt1u6FSaQkZEx\nc+bMqKiooqKiQ4cOnTt3bsKECbq/3tiSyMjIu3fvBgQEODg4sCzb1NSkUCgyMzP9/f3j4+OfffbZ\nn376qQvaD9RBeEGL6urqwsPDjSz82WefeXl5LVu2zMHBYcCAAW+//XZOTs7Fixfbu1GGYRwdHUeP\nHr1z587MzMwnT55MnDixqqqqvfWYW7t2DpgDwsvyuu03R+/YsUOhUBhZ+MGDBz169ND2xcfHhxBS\nWFjYmQa8/vrrc+fOVSgUW7du7Uw95tCunQPmgPCyAJZlU1JS+vTpIxKJHBwcli1bpl304YcfSiQS\ne3t7hUKxZMmSnj175ufnsyy7cePGvn37ikQiJyenyZMn5+XlceU//vhjsVjs7u6+YMGCHj16iMXi\n8PBw3fOdVtaNjY0VCoWenp7c5F/+8hepVMowTGlpKSEkLi5uyZIld+7cYRgmMDCwzU7J5XLdg5m7\n4SWXy7nJ48ePy2SypKSk9u6ruXPnEkKOHTtG9c4Bs2DBpAghGRkZrZdZtWoVwzAbNmyoqKhQqVRp\naWmEkMuXL2uXEkIWL168efPmKVOm3Lx5MyEhQSgU7tq1q7KyMjc3d+DAga6uro8fP+bKx8TESKXS\nGzduPH369Pr169wt8/v373NLW1935syZHh4e2oalpKQQQkpKSrjJyMjIgIAAIzt+9uxZgUDw8ccf\nK5XKa9eu9e3b96WXXtIuzcrKsre3T0xMbGl17T0vPUqlkhDi4+ND9c7JyMjAsWZy2KEm1mZ4qVQq\niUQybtw47Zz09HTD8Kqrq9OWt7Ozi46O1pb/8ccfCSHaIIiJidE97C9dukQIeffdd41Z14THJ8uy\nq1ev1r4pent7P3jwwPh1WwovlmW5u2Dc35TuHISXOeCysasVFBSoVKqxY8caWf769es1NTWDBg3S\nzhk8eLBQKGzpXvigQYMkEgl3+dPedTtj1apV27ZtO3XqVE1Nzd27d8PDw4cNG/bgwYNOVltbW8uy\nrEwma3YpLTsHzAHh1dWKiooIIW5ubkaWr6ysJITY2dnpznR0dKyurm5pFZFIVFJS0rF1O+bRo0fJ\nycl//vOfX3zxRalU6u/vv3379uLiYu5spTNu3bpFCAkKCmp2KRU7B8wE4dXVxGIxIaS+vt7I8o6O\njoQQvSOqsrLS29u72fIajUa7tL3rdtjt27cbGxu9vLy0c2QymbOz8/Xr1ztZ8/HjxwkhEyZMaHYp\nFTsHzATh1dVCQkJ4PF52drbx5e3s7HQf1Lx48aJarX7++eebLX/27FmWZcPCwoxZl8/nazSaDvZE\nB3fAP3r0SDunurq6vLyce2Ciwx4/fpyamurt7f3HP/6x2QJU7BwwE4RXV3Nzc4uMjNy3b9+OHTuU\nSmVubu62bdtaKS8Wi5csWXLgwIHdu3crlcqrV68uXLiwR48eMTEx5yOvggAAHohJREFU2jJNTU0V\nFRUNDQ25ublxcXG+vr7cEwZtrhsYGFheXn7w4EGNRlNSUqL3WJazs3NxcfG9e/eqq6tbP4z9/f3H\njBmzffv2c+fO1dXVPXjwgNvEm2++yRU4duxYm49KsCxbU1PT1NTEsmxJSUlGRsbw4cNtbGwOHjzY\n0j0vKnYOmItFPy6wQsSIRyWqq6vnz5/v4uJiZ2c3YsSIhIQEQoi3t/eVK1eSk5NtbW0JIT4+Prt2\n7eLKNzU1paSk9O7dWyAQODk5RUREcM83cWJiYgQCQc+ePfl8vkwmmzx58p07d7RLW1+3rKxszJgx\nYrHY39//b3/7G/fEWWBgIPcwwS+//OLn52draztixAjtAwQtKS0tjYuLCwwMFIlEdnZ2w4cP/+qr\nr7RLjx49am9vv3btWsMVDx8+3K9fP4lEIhQKud+p5T5eHDJkSGJiYllZmbYkvTsHnzaaA8OyrOWS\n0woxDJORkTF16tQu2+KCBQv27t1bVlbWZVukSDfZOZmZmdOmTcOxZlq4bLQGjY2Nlm5C94WdY60Q\nXtC2vLw8pmXR0dGWbiD8HiG86LZy5cqdO3dWVVX5+/vv27fPTFsJCgpq5dbDnj17zLTdTuqanQOW\ngnteJtb197yg+8M9L3PAmRcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQ\nCeEFAFRCeAEAlRBeAEAlhBcAUIlv6QZYofPnz1u6CdC9YEiYA74Sx8QYhrF0E6CbwrFmWggvMBfu\nS80yMzMt3RCwTrjnBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEA\nlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEA\nlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFTiW7oB\nYD2ys7MvXLignczLyyOEJCcna+eEhYWNGjXKAi0Da8SwLGvpNoCV+O9//zt+/HiBQMDj6Z/RNzU1\naTSaEydOjBs3ziJtA+uD8AKTaWxs9PDwKCsra3apk5OTQqHg83GyD6aBe15gMjY2NjNnzhQKhYaL\nhELh7NmzkVxgQggvMKXp06er1WrD+Wq1evr06V3fHrBiuGwEE/Pz87t//77eTG9v7/v37zMMY5Em\ngVXCmReY2KxZswQCge4coVD4xhtvILnAtHDmBSZ28+bN4OBgvZlXr14NCQmxSHvAWiG8wPSCg4Nv\n3rypnQwKCtKdBDAJXDaC6c2ZM0d75SgQCN544w3LtgesEs68wPTu37/fq1cvbmgxDHP37t1evXpZ\nulFgbXDmBabn6+s7aNAgHo/HMMzgwYORXGAOCC8wizlz5vB4PBsbm9mzZ1u6LWCdcNkIZlFSUtKj\nRw9CyMOHDz08PCzdHLBCCK9uAc9A0QVHTXeA/zXrLuLi4oYNG2bpVphSdnY2wzAvvPCCpRtiSufP\nn9+0aZOlWwGEILy6j2HDhk2dOtXSrTCll19+mRAik8ks3RATQ3h1EwgvMBfriy3oVvBpIwBQCeEF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeFF\npfnz59vb2zMMk5OTY+m2/B+NRpOQkCCXy4VCYc+ePZcuXVpXV2fMivv375fL5YwOoVDo7u4+evTo\nlJSUiooKc7ccaMVCN0AIycjIaNcq6enphJDLly+bqUnttWjRIrFYnJ6erlQqz5w5I5PJZsyYYfzq\nAQEBDg4OLMs2NTVVVFScOXNm7ty5DMP06NHj0qVLZmt1u2VkZOCo6SZw5gUmcPfu3a1bt86ZMyc6\nOtre3n706NGxsbFffvllB35rlmEYR0fH0aNH79y5MzMz88mTJxMnTqyqqjJHs4FqCC9adauvvb90\n6VJTU9PQoUO1c7ivUf3mm286U+3rr78+d+5chUKxdevWzjYRrA7Cixosy6akpPTp00ckEjk4OCxb\ntkx3aWNjY0JCgq+vr62tbb9+/birmy1btkilUolEcujQoQkTJshkMm9vb+56k5OdnT1kyBCJRCKT\nyUJDQ5VKZUtVtY7H4xFCbG1ttXN69+5NCNGeeR0/flwmkyUlJbW313PnziWEHDt2rDt0E7oXS1+3\nAssad89r1apVDMNs2LChoqJCpVKlpaURnXteS5cuFYlE+/btq6ioWLlyJY/H424VrVq1ihBy6tSp\nqqoqhUIxcuRIqVSqVqtZlq2pqZHJZMnJyXV1dY8fP54yZUpJSUkrVbUiNzeXEPLOO+9o5zQ0NBBC\nIiIiuMmsrCx7e/vExMSWatDe89LDBY2Pj0936CaLe17dCV6GbqHN8FKpVBKJZNy4cdo5ujfs6+rq\nJBJJdHS0trBIJFq0aBH7v6O6rq6OW8RFXkFBAcuy165dI4RkZWXpbqiVqlr38ssvOzs7nzp1qq6u\n7tGjR5mZmQzDvPLKK0bugZbCi2VZ7i5YN+kmwqv7wGUjHQoKClQq1dixY5tdmp+fr1KpQkJCuElb\nW1tPT8+8vDzDkkKhkBCi0WgIIXK53N3dfdasWWvWrLl37157q9KzZ8+eqKioOXPmODs7Dx8+/Kuv\nvmJZ1sXFpd1d/a3a2lqWZbnf8ugO3YTuA+FFh6KiIkKIm5tbs0tra2sJIatXr9Y+KlVYWKhSqVqv\n09bW9vTp0yNGjEhKSpLL5dHR0XV1dR2rihDi4OCwdevWoqIilUp1586dDRs2EEK8vLza21M9t27d\nIoQEBQV1k25C94HwooNYLCaE1NfXN7uUC7XU1FTdk+rz58+3We2zzz575MiR4uLi+Pj4jIyM9evX\nd7gqPZcuXSKEjBkzpr0r6jl+/DghZMKECaRbdhMsCOFFh5CQEB6Pl52d3exSHx8fsVjc3qfti4uL\nb9y4QQhxc3Nbt27dwIEDb9y40bGqDG3fvt3f33/UqFGdqeTx48epqane3t5//OMfSbfsJlgQwosO\nbm5ukZGR+/bt27Fjh1KpzM3N3bZtm3apWCyeN29eenr6li1blEplY2NjUVHRo0ePWq+zuLh4wYIF\neXl5arX68uXLhYWFYWFhHauKEDJkyJDCwsKGhoZ79+4tXbr05MmTO3bs4O49EUKOHTvW5qMSLMvW\n1NQ0NTWxLFtSUpKRkTF8+HAbG5uDBw9y97y6QzehGzHP5wDQPsSIRyWqq6vnz5/v4uJiZ2c3YsSI\nhIQEQoi3t/eVK1dYlq2vr4+Pj/f19eXz+VzSXb9+PS0tTSKREEJ69+59586dbdu2cSng5+d369at\ne/fuhYeHOzk52djYeHl5rVq1qqGhoaWq2uzCuHHjHB0d+Xy+k5PTxIkT9R47OHr0qL29/dq1aw1X\nPHz4cL9+/SQSiVAo5J4X4z5eHDJkSGJiYllZmW5hi3cTnzZ2HwzLspZLTvg/DMNkZGRMnTrV0g2B\nNmRmZk6bNg1HTXeAy0YAoBLCC9qWl5fHtCw6OtrSDYTfI76lGwAUCAoKwoUSdDc48wIAKiG8AIBK\nCC8AoBLCCwCohPACACohvACASggvAKASwgsAqITwAgAqIbwAgEoILwCgEsILAKiE8AIAKiG8AIBK\n+CbVboFhGEs3AdoBR013gO/z6ha4b0a3MqmpqYSQt956y9INAeuEMy8wF+4r+TMzMy3dELBOuOcF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEF\nAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFCJb+kGgPUoLS1VKpXaydraWkLI3bt3\ntXNkMpmrq6sFWgbWiGFZ1tJtACuxY8eO+fPnt1Lg888/f/PNN7usPWDdEF5gMhUV/6+9ew+Kquzj\nAP4c2RuwN0AUBEFdGHGCLl4JbYQaCscikLiKROmElwkpNBtNJ83LGGM4KaNZjI0guIs5VibmaKZN\nJupoQYqaGKAhcr8uyi573j/O2w6hwCJnOTzr9/MX+5znPOfHk+fbOc8edhtHjx5tMBgeuVUsFt+7\nd8/JyWmIqwJbhTUv4I2Tk1NYWJhI9Ii1CJFINGfOHCQX8AjhBXxKTEzs6up6uL2rqysxMXHo6wEb\nhttG4NP9+/ddXFz0en2Pdnt7+7q6OgcHB0GqApuEKy/gk0wmi4yMFIvF3RvFYnFUVBSSC/iF8AKe\nJSQk9FizNxgMCQkJQtUDtgq3jcAzo9E4atSoxsZGc4tara6pqelxOQYwSLjyAp6JRKK4uDiJRMK9\nFIvFCQkJSC7gHcIL+BcfH9/Z2cn9bDAY4uPjha0HbBJuG4F/LMt6enpWVVURQtzc3KqqqhiGEboo\nsDW48gL+MQyTmJgokUjEYnFSUhKSC6wB4QVWwd054n1GsB58qgTPoqOjhS5huJDL5YSQjRs3Cl3I\ncFFQUCB0CTYFa148YxgmMDDQ09NT6EKEV1paSgiZNGmS0IUI786dO+fOncO5xi+EF88YhtFqtTEx\nMUIXIryysjJCiEajEboQ4el0utjYWJxr/MJtI1gLYgusCgv2AEAlhBcAUAnhBQBUQngBAJUQXgBA\nJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhJfAFi1apFAoGIb5/fffha5l\nUAwGw7p16yZMmCCRSDw8PFasWNHR0WHJjt98882ECROYbiQSyahRo4KDgzMyMrp/hRrAf7DAK0KI\nVqsd0C75+fmEkMuXL1uppKGxdOlSmUyWn5/f0tJy6tQppVKZkJBg+e4ajUalUrEsazKZGhsbT506\nlZyczDCMu7v7hQsXrFb1ENFqtTjXeIcrL+hVR0dHUFCQJT1v3bq1e/fupKSkuLg4hUIRHBycmpqa\nl5fHfZjqgDAMo1arg4OD9+7dq9Pp7t27N3fu3Obm5oGXb12WTw5YCcJLeMP2y3Wys7Nramos6Xnh\nwgWTyTRjxgxzS1hYGCHkxx9/HEwBb7zxRnJyck1Nze7duwczjjVYPjlgJQgvAbAsm5GRMXHiRKlU\nqlKpVq5cad706aefOjg4KBSKmpqa9PR0Dw+P69evsyz72WefTZo0SSqVOjk5RUREXLt2jev/+eef\ny2SyUaNGLV682N3dXSaTBQUFFRUVdT9Wb/umpqZKJBI3Nzfu5bJlyxwdHRmGqaurI4SkpaWlp6eX\nlZUxDOPj49P3bzRixAhCiL29vbnF19eX/Psx9oSQY8eOKZXKTZs2DXSukpOTCSGFhYX0Tg5Yi6A3\nrTaIWLDmtWbNGoZhtm3b1tjYqNfrs7KySLc1rzVr1hBCli9fvmPHjnnz5pWWlq5bt04ikeTk5DQ1\nNRUXF0+ePHnkyJHV1dVc/5SUFEdHx6tXr96/f//KlSvTpk1TKBSVlZXc1r73nT9//ujRo82FZWRk\nEEJqa2u5l1FRURqNxpLfuri4mBCydu1ac4vRaCSEREZGci+PHDmiUCg2bNjQ2wjmNa8eWlpaCCFj\nx46ld3JYrHlZByaUZ/2Gl16vd3BwCA0NNbf0WLDnzs+Ojg5zf7lcHhcXZ+5//vx5Qog5CFJSUrqf\n9hcuXCCErF+/3pJ9eTw/w8LCnJ2dT5482dHRcffuXZ1OxzDMq6++auHuvYUXy7LcKhj3M6WTg/Cy\nBnwBx1C7efOmXq9/6aWXLOx/5cqVtra2qVOnmlumTZsmkUi63/50N3XqVAcHB+72Z6D7DsaBAwdW\nrVqVlJTU0NDg7u4+Y8YMlmVdXFwGOWx7ezvLskql8pFbaZkcsAaE11C7c+cOIcTV1dXC/k1NTeTf\nL3A1U6vVra2tve0ilUpra2sfb9/HplKpui+r3717Nz8/f8yYMYMc9saNG4QQPz+/R26lZXLAGrBg\nP9RkMhkh5MGDBxb2V6vVhJAeZ1RTU1Nv32trMBjMWwe6L4+4G7SQkJBBjnPs2DFCyJw5cx65ldLJ\nAV4gvIaav7//iBEjTp8+bXl/uVx+8eJFc0tRUVFnZ+eUKVMe2f/nn39mWTYwMNCSfUUikcFgeMzf\npE9ffvnl+PHjZ8+ePZhBqqurMzMzPT0933777Ud2oHRygBcIr6Hm6uoaFRV18ODB7OzslpaW4uLi\nPXv29NFfJpOlp6cfOnQoNze3paWlpKRkyZIl7u7uKSkp5j7cU+lGo7G4uDgtLc3Ly4t7wqDffX18\nfBoaGg4fPmwwGGpraysqKrof2tnZuaqqqry8vLW1td/TePr06RUVFUajsby8fMWKFSdOnMjOzpZI\nJNzWwsLCfh+VYFm2ra3NZDKxLFtbW6vVamfOnGlnZ3f48OHe1rxomRywCiHfLbBFxIJHJVpbWxct\nWuTi4iKXy2fNmrVu3TpCiKen5x9//LF161buaamxY8fm5ORw/U0mU0ZGhq+vr1gsdnJyioyM5J5v\n4qSkpIjFYg8PD5FIpFQqIyIiysrKzFv73re+vj4kJEQmk40fP/7dd9/lnjjz8fHhHia4dOmSt7e3\nvb39rFmzzA8Q9CY0NFStVotEIicnp7lz5/b4m56jR48qFIqNGzc+vON333339NNPOzg4SCQS7nkx\n7u3F6dOnb9iwob6+3tyT3snBu43WwLAsK1xy2iCGYbRabUxMzJAdcfHixQUFBfX19UN2RIoMk8nR\n6XSxsbE41/iF20Zb0NXVJXQJwxcmx1YhvKB/165dY3oXFxcndIHwJEJ40W316tV79+5tbm4eP378\nwYMHrXQUPz+/PpYeDhw4YKXjDtLQTA4IBWtePBv6NS8Y/rDmZQ248gIAKiG8AIBKCC8AoBLCCwCo\nhPACACohvACASggvAKASwgsAqITwAgAqIbwAgEoILwCgEsILAKiE8AIAKuFTJXjGMExgYCC+gQa6\nu3Pnzrlz53Cu8QvhxbPo6GihSxguuC/m6f6trk+4goICoUuwKQgvsBbuQ810Op3QhYBtwpoXAFAJ\n4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ\n4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ\n4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUIlhWVboGsBGfP3119u3b+/q6uJe\n1tbWEkJcXV25l3Z2dmlpacnJyUKVBzYG4QW8uX79up+fXx8dSktL++4AYDncNgJvJk6cGBAQwDDM\nw5sYhgkICEByAY8QXsCnpKQkOzu7h9tFItGbb7459PWADcNtI/CpqqrK09Pz4X9UDMNUVlZ6enoK\nUhXYJFx5AZ/GjBkTFBQ0YsR//l2NGDEiKCgIyQX8QngBzxYsWNBj2YthmKSkJKHqAVuF20bgWUND\nw+jRo41Go7nFzs7u3r17Li4uAlYFtgdXXsAzZ2fn0NBQkUjEvbSzswsNDUVyAe8QXsC/xMREk8nE\n/cyy7IIFC4StB2wSbhuBf+3t7SNHjrx//z4hRCqV1tXVyeVyoYsCW4MrL+Cfo6NjeHi4WCwWiUQR\nERFILrAGhBdYxfz5841GY1dXV0JCgtC1gG0SCV0AHe7cuXP27Fmhq6BJV1eXTCZjWbatrU2n0wld\nDk3wTJyFsOZlEZ1OFxsbK3QV8ETQarUxMTFCV0EBXHkNAIJ+QE6dOsUwTHBwsNCF0OSRf9YOj4Tw\nAmuZPXu20CWALUN4gbX0+AtHAH7hnxcAUAnhBQBUQngBAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4A\nQCWEFwBQCeEFAFRCeAEAlRBeAEAlhNcwcvToUZVK9f333w/zMXtjMBg2b97s4+MjkUjUarW/v395\nefmARsjLy2MYJigoaDBl0D6NYCGE1zBijc8LG8rPIIuNjd23b9/+/fv1en1paalGo2lraxvQCHl5\neRqN5rfffrt58+Zjl0H7NIKlWLCAVqu1xlzp9frnn39++I9pifz8fIZhiouLH3uEurq68ePH5+bm\nEkLWrl1r+Y62NI2EEK1WO/THpRGuvISUnZ1dU1Mz/Me0xK5duyZPnhwQEPDYI+h0urlz54aHh8tk\nspycHNbiix1bmkYYAKHTkw4WXnmdOXNm0qRJSqVSKpX6+/sfO3bMvGnfvn1TpkyRSqUODg7e3t4b\nNmxYvny5RCLh/itoNJpffvll7NixhJAdO3awLOvn50cIYRhm8uTJ7e3tLMuuXLmSG3nv3r29Havv\nMVmWNZlM27Zt8/Pz45alXn/99dLSUm5TVlaWg4ODvb394cOHw8LCFAqFh4dHXl6eJfPz4MEDiUSy\ncOHC3joUFhYqFIqNGzf2McisWbN++uknlmXDw8MJIadPn364j21PI4srr4FAeFnEwvAqKCj4+OOP\nGxoa6uvrAwMDXVxcuPbMzExCyJYtW+rr6xsaGr744ov58+ezLBsVFaXRaMy7375923yGGI3GcePG\neXl5GY1Gc4f33nsvMzOz72P1MSbLsuvWrZNIJDk5OU1NTcXFxZMnTx45cmR1dTW3dc2aNYSQkydP\nNjc319TUvPDCC46Ojp2dnf3+4n///Tch5Nlnnw0ODnZzc5NKpX5+fjt37jSZTFyHI0eOKBSKDRs2\n9DZCRUWFq6sr98vm5OQQQh6OQpufRhbhNRAIL4s8xprX5s2bCSE1NTWdnZ1qtTokJMS8yWg0bt++\nne3vDOHOVZ1Ox71sb2/38vJqbm7u41h9j6nX6+VyeVxcnHnr+fPnCSHmTOHOuo6ODu5lVlYWIeTm\nzZv9/rIlJSWEkNDQ0F9//bW+vr6pqenDDz8khOTm5loyVyzLbtmy5a233uJ+bm5ulkqlSqVSr9eb\nOzwJ08givAYCa17WIhaLCSFdXV3FxcVNTU2vvPKKeZOdnd3y5cv7HWHRokUqlWr79u3cy9zc3IiI\nCKVS2cex+h7wypUrbW1tU6dONbdMmzZNIpEUFRU9sj9362QwGPotVSqVEkKeeuqpoKAgZ2dnlUq1\nfv16lUq1Z8+efvfl5OXlzZs3j/tZqVS+/PLLLS0t3377rbnDkzCNMCAILz798MMPwcHBrq6uUqn0\ngw8+4BpbWloIIWq1eqCjyeXyd9555+zZs9z/2Hft2pWamtr3sfrW1NTEDdu9Ua1Wt7a2DrS2Htzd\n3QkhdXV15haJROLt7V1WVmbJ7n/++WdJSclrr73G/It7omrfvn3mPk/CNMKAILx4U1lZGRkZ6ebm\nVlRU1NzcvHXrVq59zJgx5L8ntuVSU1PFYnFmZuaZM2fGjh2r0Wj6PlbfuDO/xznW1NQ0+O9nlsvl\nvr6+V69e7d5oNBpVKpUlu+/fvz8+Pr77HUFDQ4O9vf3x48erq6u5Pk/CNMKAILx4U1JSYjAYli5d\nOmHCBJlMZv720HHjxjk7Ox8/fvwxxvT09IyJiTl48ODatWvT0tL6PVbf/P395XL5xYsXzS1FRUWd\nnZ1Tpkx5jNp6iI2NvXz58q1bt7iXer2+oqLCkicnWJY9cODAsmXLujc6OTlFR0d3dXXl5eVxLU/I\nNILlEF688fLyIoScOHHi/v37f/31l3kFRCqVrl69+syZM6mpqf/884/JZGptbeUuUpydnauqqsrL\ny1tbW3tbE0lPTzcajY2NjS+++GK/x+p7TJlMlp6efujQodzc3JaWlpKSkiVLlri7u6ekpAz+13//\n/fe9vb2Tk5MrKyvr6+tXrVrV0dHBLdsTQgoLC5VK5aZNmx7e8ezZs0qlcubMmT3alyxZQrrdOT4h\n0wgDINAbBZSx8N3GVatWOTs7q9Xq6OjonTt3EkI0Gk1lZSXLsjt37gwICJDJZDKZ7LnnnsvKymJZ\n9tKlS97e3vb29rNmzfroo4/c3NwIIQ4ODuHh4d2HDQkJ+eqrryw8Vt9jmkymjIwMX19fsVjs5OQU\nGRl5/fp1bkDuASVCiK+vb1lZ2Z49e7hVbW9v7xs3blgyS7dv346Pj3dycpJKpdOnTy8sLDRvOnr0\n6COf81q4cKGjo6NIJHrmmWcuXbpkbv/kk0+4dTRCiIeHBzddT8I0ErzbaDGGxR9tWUCn08XGxmKu\nwNoYhtFqtTExMUIXQgHcNgIAlRBe0L9r164xvYuLixO6QHgSiYQuACjg5+eHW2YYbnDlBQBUQngB\nAJUQXgBAJYQXAFAJ4QUAVEJ4AQCVEF4AQCWEFwBQCeEFAFRCeAEAlRBeAEAlhBcAUAnhBQBUQngB\nAJXwkTgDoNPphC4BAP4P4TUAsbGxQpcAAP+Hz7AHACphzQsAqITwAgAqIbwAgEoILwCg0v8AKAdJ\ndqXB1/AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UokDWKIIXSgz",
        "colab_type": "text"
      },
      "source": [
        "Using the adam optimizer and binary_crossentropy as our loss function. We tried categorical_crossentropy as a loss function as well, but the accuracy was poor in comparison. We saw a similar trend with the SGD optimizer and decided to go with adam and binary_crossentropy. We tried random_normal and zeros initializers, but did not get better results. Hence, we decided to stick with the default glorot_uniform initializer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HRCEV6ltpMwF",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GnycUmNoNuUM"
      },
      "source": [
        "## Train the model using a single batch\n",
        "To start off, we trained the model on a single batch of training data. The batch size was 64. We ran the training for 200 epochs. Max. accuracy was around 96%. Increasing number of epochs did not increase accuracy by much.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "820fdfe2-2991-492c-f8f7-e4a37f2a11c7",
        "id": "UfJ0vmpMda3w",
        "colab": {}
      },
      "source": [
        "for epoch in range(200):\n",
        "    metrics = model.train_on_batch(example_input_batch, example_target_batch)\n",
        "    print(\"Epoch: %d, Loss: %f, Accuracy: %f\" % (epoch, metrics[0], metrics[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /rigel/home/sh3831/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch: 0, Loss: 0.192752, Accuracy: 0.960469\n",
            "Epoch: 1, Loss: 0.192231, Accuracy: 0.960469\n",
            "Epoch: 2, Loss: 0.191397, Accuracy: 0.960469\n",
            "Epoch: 3, Loss: 0.190467, Accuracy: 0.960469\n",
            "Epoch: 4, Loss: 0.189089, Accuracy: 0.960469\n",
            "Epoch: 5, Loss: 0.188537, Accuracy: 0.960469\n",
            "Epoch: 6, Loss: 0.187759, Accuracy: 0.960469\n",
            "Epoch: 7, Loss: 0.188119, Accuracy: 0.960469\n",
            "Epoch: 8, Loss: 0.186758, Accuracy: 0.960469\n",
            "Epoch: 9, Loss: 0.185515, Accuracy: 0.960469\n",
            "Epoch: 10, Loss: 0.185102, Accuracy: 0.960469\n",
            "Epoch: 11, Loss: 0.186339, Accuracy: 0.960469\n",
            "Epoch: 12, Loss: 0.182382, Accuracy: 0.960469\n",
            "Epoch: 13, Loss: 0.185043, Accuracy: 0.960469\n",
            "Epoch: 14, Loss: 0.182039, Accuracy: 0.960469\n",
            "Epoch: 15, Loss: 0.183741, Accuracy: 0.960469\n",
            "Epoch: 16, Loss: 0.183977, Accuracy: 0.960469\n",
            "Epoch: 17, Loss: 0.184126, Accuracy: 0.960625\n",
            "Epoch: 18, Loss: 0.179760, Accuracy: 0.960625\n",
            "Epoch: 19, Loss: 0.182614, Accuracy: 0.960469\n",
            "Epoch: 20, Loss: 0.179707, Accuracy: 0.960469\n",
            "Epoch: 21, Loss: 0.178834, Accuracy: 0.960469\n",
            "Epoch: 22, Loss: 0.180064, Accuracy: 0.960469\n",
            "Epoch: 23, Loss: 0.182315, Accuracy: 0.960469\n",
            "Epoch: 24, Loss: 0.178898, Accuracy: 0.960469\n",
            "Epoch: 25, Loss: 0.179947, Accuracy: 0.960469\n",
            "Epoch: 26, Loss: 0.180826, Accuracy: 0.960469\n",
            "Epoch: 27, Loss: 0.179550, Accuracy: 0.960469\n",
            "Epoch: 28, Loss: 0.180050, Accuracy: 0.960469\n",
            "Epoch: 29, Loss: 0.182973, Accuracy: 0.960469\n",
            "Epoch: 30, Loss: 0.179131, Accuracy: 0.960469\n",
            "Epoch: 31, Loss: 0.175784, Accuracy: 0.960469\n",
            "Epoch: 32, Loss: 0.180781, Accuracy: 0.960469\n",
            "Epoch: 33, Loss: 0.175865, Accuracy: 0.960469\n",
            "Epoch: 34, Loss: 0.177379, Accuracy: 0.960469\n",
            "Epoch: 35, Loss: 0.180951, Accuracy: 0.960469\n",
            "Epoch: 36, Loss: 0.180294, Accuracy: 0.960469\n",
            "Epoch: 37, Loss: 0.178683, Accuracy: 0.960469\n",
            "Epoch: 38, Loss: 0.181629, Accuracy: 0.960469\n",
            "Epoch: 39, Loss: 0.175988, Accuracy: 0.960469\n",
            "Epoch: 40, Loss: 0.177542, Accuracy: 0.960469\n",
            "Epoch: 41, Loss: 0.173028, Accuracy: 0.960469\n",
            "Epoch: 42, Loss: 0.177744, Accuracy: 0.960469\n",
            "Epoch: 43, Loss: 0.180224, Accuracy: 0.960469\n",
            "Epoch: 44, Loss: 0.175858, Accuracy: 0.960469\n",
            "Epoch: 45, Loss: 0.178938, Accuracy: 0.960469\n",
            "Epoch: 46, Loss: 0.179812, Accuracy: 0.960469\n",
            "Epoch: 47, Loss: 0.175252, Accuracy: 0.960469\n",
            "Epoch: 48, Loss: 0.175671, Accuracy: 0.960469\n",
            "Epoch: 49, Loss: 0.182031, Accuracy: 0.960469\n",
            "Epoch: 50, Loss: 0.179308, Accuracy: 0.960469\n",
            "Epoch: 51, Loss: 0.174144, Accuracy: 0.960469\n",
            "Epoch: 52, Loss: 0.176781, Accuracy: 0.960469\n",
            "Epoch: 53, Loss: 0.175295, Accuracy: 0.960469\n",
            "Epoch: 54, Loss: 0.174576, Accuracy: 0.960469\n",
            "Epoch: 55, Loss: 0.175676, Accuracy: 0.960469\n",
            "Epoch: 56, Loss: 0.172349, Accuracy: 0.960625\n",
            "Epoch: 57, Loss: 0.176055, Accuracy: 0.960625\n",
            "Epoch: 58, Loss: 0.175955, Accuracy: 0.960469\n",
            "Epoch: 59, Loss: 0.175598, Accuracy: 0.960469\n",
            "Epoch: 60, Loss: 0.173947, Accuracy: 0.960469\n",
            "Epoch: 61, Loss: 0.174470, Accuracy: 0.960469\n",
            "Epoch: 62, Loss: 0.173775, Accuracy: 0.960469\n",
            "Epoch: 63, Loss: 0.171294, Accuracy: 0.960469\n",
            "Epoch: 64, Loss: 0.176158, Accuracy: 0.960625\n",
            "Epoch: 65, Loss: 0.171274, Accuracy: 0.960469\n",
            "Epoch: 66, Loss: 0.177255, Accuracy: 0.960469\n",
            "Epoch: 67, Loss: 0.170011, Accuracy: 0.960469\n",
            "Epoch: 68, Loss: 0.171829, Accuracy: 0.960469\n",
            "Epoch: 69, Loss: 0.171462, Accuracy: 0.960469\n",
            "Epoch: 70, Loss: 0.167351, Accuracy: 0.960469\n",
            "Epoch: 71, Loss: 0.171565, Accuracy: 0.960625\n",
            "Epoch: 72, Loss: 0.168872, Accuracy: 0.960625\n",
            "Epoch: 73, Loss: 0.174980, Accuracy: 0.960469\n",
            "Epoch: 74, Loss: 0.168797, Accuracy: 0.960625\n",
            "Epoch: 75, Loss: 0.167660, Accuracy: 0.960469\n",
            "Epoch: 76, Loss: 0.169343, Accuracy: 0.960625\n",
            "Epoch: 77, Loss: 0.172733, Accuracy: 0.960625\n",
            "Epoch: 78, Loss: 0.173073, Accuracy: 0.960469\n",
            "Epoch: 79, Loss: 0.171794, Accuracy: 0.960469\n",
            "Epoch: 80, Loss: 0.170777, Accuracy: 0.960469\n",
            "Epoch: 81, Loss: 0.169162, Accuracy: 0.960469\n",
            "Epoch: 82, Loss: 0.168407, Accuracy: 0.960469\n",
            "Epoch: 83, Loss: 0.170987, Accuracy: 0.960625\n",
            "Epoch: 84, Loss: 0.166469, Accuracy: 0.960625\n",
            "Epoch: 85, Loss: 0.167887, Accuracy: 0.960625\n",
            "Epoch: 86, Loss: 0.166827, Accuracy: 0.961094\n",
            "Epoch: 87, Loss: 0.169791, Accuracy: 0.961094\n",
            "Epoch: 88, Loss: 0.166543, Accuracy: 0.960625\n",
            "Epoch: 89, Loss: 0.170480, Accuracy: 0.960469\n",
            "Epoch: 90, Loss: 0.169126, Accuracy: 0.960469\n",
            "Epoch: 91, Loss: 0.170617, Accuracy: 0.960469\n",
            "Epoch: 92, Loss: 0.167424, Accuracy: 0.960625\n",
            "Epoch: 93, Loss: 0.165405, Accuracy: 0.960625\n",
            "Epoch: 94, Loss: 0.168108, Accuracy: 0.961094\n",
            "Epoch: 95, Loss: 0.160912, Accuracy: 0.960469\n",
            "Epoch: 96, Loss: 0.168499, Accuracy: 0.960469\n",
            "Epoch: 97, Loss: 0.170796, Accuracy: 0.960469\n",
            "Epoch: 98, Loss: 0.169203, Accuracy: 0.960469\n",
            "Epoch: 99, Loss: 0.167802, Accuracy: 0.960625\n",
            "Epoch: 100, Loss: 0.168147, Accuracy: 0.960781\n",
            "Epoch: 101, Loss: 0.167898, Accuracy: 0.960469\n",
            "Epoch: 102, Loss: 0.169157, Accuracy: 0.960781\n",
            "Epoch: 103, Loss: 0.170012, Accuracy: 0.960781\n",
            "Epoch: 104, Loss: 0.165517, Accuracy: 0.960469\n",
            "Epoch: 105, Loss: 0.165613, Accuracy: 0.960469\n",
            "Epoch: 106, Loss: 0.164742, Accuracy: 0.960625\n",
            "Epoch: 107, Loss: 0.160861, Accuracy: 0.960469\n",
            "Epoch: 108, Loss: 0.159462, Accuracy: 0.960469\n",
            "Epoch: 109, Loss: 0.163942, Accuracy: 0.960781\n",
            "Epoch: 110, Loss: 0.164272, Accuracy: 0.960781\n",
            "Epoch: 111, Loss: 0.162236, Accuracy: 0.960781\n",
            "Epoch: 112, Loss: 0.165203, Accuracy: 0.960625\n",
            "Epoch: 113, Loss: 0.160352, Accuracy: 0.960938\n",
            "Epoch: 114, Loss: 0.158621, Accuracy: 0.961094\n",
            "Epoch: 115, Loss: 0.165156, Accuracy: 0.960469\n",
            "Epoch: 116, Loss: 0.167751, Accuracy: 0.960469\n",
            "Epoch: 117, Loss: 0.158345, Accuracy: 0.960469\n",
            "Epoch: 118, Loss: 0.160765, Accuracy: 0.960625\n",
            "Epoch: 119, Loss: 0.166464, Accuracy: 0.960781\n",
            "Epoch: 120, Loss: 0.162466, Accuracy: 0.960781\n",
            "Epoch: 121, Loss: 0.159909, Accuracy: 0.961250\n",
            "Epoch: 122, Loss: 0.161554, Accuracy: 0.960625\n",
            "Epoch: 123, Loss: 0.161566, Accuracy: 0.960625\n",
            "Epoch: 124, Loss: 0.165542, Accuracy: 0.960938\n",
            "Epoch: 125, Loss: 0.156590, Accuracy: 0.960938\n",
            "Epoch: 126, Loss: 0.164693, Accuracy: 0.960625\n",
            "Epoch: 127, Loss: 0.160426, Accuracy: 0.961094\n",
            "Epoch: 128, Loss: 0.161499, Accuracy: 0.960469\n",
            "Epoch: 129, Loss: 0.160494, Accuracy: 0.960781\n",
            "Epoch: 130, Loss: 0.160905, Accuracy: 0.960625\n",
            "Epoch: 131, Loss: 0.158882, Accuracy: 0.960781\n",
            "Epoch: 132, Loss: 0.157913, Accuracy: 0.961563\n",
            "Epoch: 133, Loss: 0.157846, Accuracy: 0.960781\n",
            "Epoch: 134, Loss: 0.157261, Accuracy: 0.960781\n",
            "Epoch: 135, Loss: 0.162735, Accuracy: 0.961094\n",
            "Epoch: 136, Loss: 0.161684, Accuracy: 0.960781\n",
            "Epoch: 137, Loss: 0.160279, Accuracy: 0.960781\n",
            "Epoch: 138, Loss: 0.155450, Accuracy: 0.960781\n",
            "Epoch: 139, Loss: 0.159336, Accuracy: 0.960625\n",
            "Epoch: 140, Loss: 0.153742, Accuracy: 0.961094\n",
            "Epoch: 141, Loss: 0.154714, Accuracy: 0.960938\n",
            "Epoch: 142, Loss: 0.157417, Accuracy: 0.960469\n",
            "Epoch: 143, Loss: 0.159223, Accuracy: 0.961563\n",
            "Epoch: 144, Loss: 0.152217, Accuracy: 0.960625\n",
            "Epoch: 145, Loss: 0.157869, Accuracy: 0.961094\n",
            "Epoch: 146, Loss: 0.157863, Accuracy: 0.960938\n",
            "Epoch: 147, Loss: 0.157981, Accuracy: 0.960469\n",
            "Epoch: 148, Loss: 0.159096, Accuracy: 0.960781\n",
            "Epoch: 149, Loss: 0.159424, Accuracy: 0.960625\n",
            "Epoch: 150, Loss: 0.150733, Accuracy: 0.960625\n",
            "Epoch: 151, Loss: 0.160684, Accuracy: 0.961406\n",
            "Epoch: 152, Loss: 0.158715, Accuracy: 0.961563\n",
            "Epoch: 153, Loss: 0.161534, Accuracy: 0.960781\n",
            "Epoch: 154, Loss: 0.159507, Accuracy: 0.960625\n",
            "Epoch: 155, Loss: 0.155275, Accuracy: 0.960781\n",
            "Epoch: 156, Loss: 0.152980, Accuracy: 0.960938\n",
            "Epoch: 157, Loss: 0.154061, Accuracy: 0.961094\n",
            "Epoch: 158, Loss: 0.156032, Accuracy: 0.961250\n",
            "Epoch: 159, Loss: 0.151506, Accuracy: 0.960781\n",
            "Epoch: 160, Loss: 0.153346, Accuracy: 0.961250\n",
            "Epoch: 161, Loss: 0.155652, Accuracy: 0.961094\n",
            "Epoch: 162, Loss: 0.154072, Accuracy: 0.960625\n",
            "Epoch: 163, Loss: 0.156245, Accuracy: 0.961094\n",
            "Epoch: 164, Loss: 0.151301, Accuracy: 0.961250\n",
            "Epoch: 165, Loss: 0.152273, Accuracy: 0.960781\n",
            "Epoch: 166, Loss: 0.153545, Accuracy: 0.961094\n",
            "Epoch: 167, Loss: 0.154572, Accuracy: 0.960781\n",
            "Epoch: 168, Loss: 0.154237, Accuracy: 0.961875\n",
            "Epoch: 169, Loss: 0.155012, Accuracy: 0.961094\n",
            "Epoch: 170, Loss: 0.149481, Accuracy: 0.961094\n",
            "Epoch: 171, Loss: 0.154485, Accuracy: 0.960938\n",
            "Epoch: 172, Loss: 0.152406, Accuracy: 0.960938\n",
            "Epoch: 173, Loss: 0.156658, Accuracy: 0.961563\n",
            "Epoch: 174, Loss: 0.150074, Accuracy: 0.961094\n",
            "Epoch: 175, Loss: 0.149242, Accuracy: 0.961563\n",
            "Epoch: 176, Loss: 0.152971, Accuracy: 0.961406\n",
            "Epoch: 177, Loss: 0.155305, Accuracy: 0.961094\n",
            "Epoch: 178, Loss: 0.152247, Accuracy: 0.962031\n",
            "Epoch: 179, Loss: 0.148106, Accuracy: 0.961094\n",
            "Epoch: 180, Loss: 0.156160, Accuracy: 0.961719\n",
            "Epoch: 181, Loss: 0.149193, Accuracy: 0.961875\n",
            "Epoch: 182, Loss: 0.147011, Accuracy: 0.961563\n",
            "Epoch: 183, Loss: 0.154133, Accuracy: 0.961406\n",
            "Epoch: 184, Loss: 0.156311, Accuracy: 0.961406\n",
            "Epoch: 185, Loss: 0.158648, Accuracy: 0.961563\n",
            "Epoch: 186, Loss: 0.147749, Accuracy: 0.961875\n",
            "Epoch: 187, Loss: 0.150126, Accuracy: 0.961563\n",
            "Epoch: 188, Loss: 0.150941, Accuracy: 0.961719\n",
            "Epoch: 189, Loss: 0.146087, Accuracy: 0.961719\n",
            "Epoch: 190, Loss: 0.151398, Accuracy: 0.960938\n",
            "Epoch: 191, Loss: 0.150183, Accuracy: 0.961094\n",
            "Epoch: 192, Loss: 0.145138, Accuracy: 0.961563\n",
            "Epoch: 193, Loss: 0.145885, Accuracy: 0.961875\n",
            "Epoch: 194, Loss: 0.146251, Accuracy: 0.961406\n",
            "Epoch: 195, Loss: 0.152722, Accuracy: 0.961875\n",
            "Epoch: 196, Loss: 0.147691, Accuracy: 0.961875\n",
            "Epoch: 197, Loss: 0.148165, Accuracy: 0.961563\n",
            "Epoch: 198, Loss: 0.146301, Accuracy: 0.961719\n",
            "Epoch: 199, Loss: 0.153080, Accuracy: 0.961719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86EOo31bxxB9",
        "outputId": "4bd1cc95-f8d8-4d3d-869e-b4a7a44e3ba4",
        "colab": {}
      },
      "source": [
        "for prediction, answer in zip(model.predict(x=example_input_batch), example_target_batch):\n",
        "  print(prediction, answer.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02932125 0.05382309 0.05282445 0.02237244 0.01721556 0.00770017\n",
            " 0.01046339 0.01234722 0.02069676 0.03648932 0.02907567 0.00961721\n",
            " 0.01392362 0.01194546 0.00770017 0.01654677 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00882342 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017 0.00770017\n",
            " 0.00770017 0.00770017 0.00770017 0.00770017] [0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00835517 0.07600346 0.00835517 0.01820081 0.01268999 0.05498028\n",
            " 0.01825764 0.00835517 0.00835517 0.00835517 0.00835983 0.01516983\n",
            " 0.01669935 0.01013833 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.01431985 0.00835517 0.00835517 0.00913278 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.01079263 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517 0.00835517\n",
            " 0.00835517 0.00835517 0.00835517 0.00835517] [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01829977 0.07573861 0.06567606 0.06479909 0.01343109 0.00723027\n",
            " 0.01896287 0.00723027 0.00723027 0.0100868  0.00723027 0.00723027\n",
            " 0.01963242 0.00723027 0.01460848 0.01437963 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00929467 0.03529127 0.00723027 0.00723027\n",
            " 0.00964218 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00835411 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027 0.00723027\n",
            " 0.00723027 0.00723027 0.00723027 0.00723027] [0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01443561 0.01348044 0.04386636 0.03451666 0.04398036 0.00837992\n",
            " 0.01770018 0.02560221 0.00757618 0.01957603 0.00757618 0.00757618\n",
            " 0.00997153 0.01482465 0.00757618 0.00884129 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.04615824 0.02262857 0.00757618\n",
            " 0.01327192 0.00757618 0.00757618 0.00757618 0.00757618 0.01063421\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.01771376 0.00757618 0.01077447 0.00997279 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618 0.00757618\n",
            " 0.00757618 0.00757618 0.00757618 0.00757618] [0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00998603 0.05231825 0.00800622 0.04989047 0.00800622 0.00800622\n",
            " 0.0138188  0.00800622 0.00800622 0.00800622 0.01854014 0.03751599\n",
            " 0.00800622 0.02621105 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.01550347 0.00800622 0.00964168 0.00800622 0.00800622 0.009717\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.01093316 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.01581869\n",
            " 0.00800622 0.03356406 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622 0.00800622\n",
            " 0.00800622 0.00800622 0.00800622 0.00800622] [0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0082446  0.01346682 0.0082446  0.00883536 0.0082446  0.0082446\n",
            " 0.0082446  0.00989441 0.0082446  0.0082446  0.0082446  0.04813809\n",
            " 0.0082446  0.02296272 0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0125159  0.0082446  0.01057764 0.00952849 0.0088248  0.01555698\n",
            " 0.03085505 0.0082446  0.0082446  0.03021083 0.0082446  0.00839587\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.01052787 0.0082446  0.01437255 0.0082446\n",
            " 0.00944012 0.0082446  0.0082446  0.01418122 0.0082446  0.01448099\n",
            " 0.0082446  0.03942145 0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446  0.0082446  0.0082446\n",
            " 0.0082446  0.0082446  0.0082446  0.0082446 ] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0101313  0.02640766 0.00808573 0.01395848 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.0090761\n",
            " 0.02245843 0.02267295 0.04495076 0.00808573 0.00808573 0.00808573\n",
            " 0.01828886 0.00808573 0.04335434 0.01067543 0.00808573 0.00808573\n",
            " 0.01023628 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.04591976 0.00808573\n",
            " 0.00808573 0.00808573 0.01400402 0.00808573 0.01165526 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00892341\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573 0.00808573\n",
            " 0.00808573 0.00808573 0.00808573 0.00808573] [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0074929  0.00994945 0.03576009 0.01805263 0.01578638 0.02997062\n",
            " 0.02507029 0.01424858 0.0074929  0.01020886 0.0074929  0.01222966\n",
            " 0.0074929  0.01883295 0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.00825436 0.0074929  0.0074929  0.03138106 0.04564764 0.0074929\n",
            " 0.01493868 0.0074929  0.0074929  0.0074929  0.0074929  0.0205805\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.00915379 0.0074929\n",
            " 0.03231987 0.0074929  0.0074929  0.02713942 0.0074929  0.0074929\n",
            " 0.0074929  0.01354996 0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929  0.0074929  0.0074929\n",
            " 0.0074929  0.0074929  0.0074929  0.0074929 ] [0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01025499 0.05713736 0.05684518 0.02155668 0.02158549 0.0134251\n",
            " 0.02121161 0.01085614 0.00949724 0.02544445 0.01021809 0.01314864\n",
            " 0.00803348 0.0102387  0.00803348 0.00913745 0.00803348 0.00803348\n",
            " 0.02417675 0.00803348 0.00910535 0.00938166 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348 0.00803348\n",
            " 0.00803348 0.00803348 0.00803348 0.00803348] [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01076907 0.06355014 0.00754923 0.01586394 0.00754923 0.00997179\n",
            " 0.00754923 0.01004321 0.01719349 0.00754923 0.01811328 0.07550748\n",
            " 0.01413048 0.06022925 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.01166719\n",
            " 0.00754923 0.00754923 0.00754923 0.00833051 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.01239551 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.01684634 0.00754923 0.00939327\n",
            " 0.00754923 0.0118598  0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923 0.00754923\n",
            " 0.00754923 0.00754923 0.00754923 0.00754923] [0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0154031  0.03953974 0.0087601  0.01836496 0.01487219 0.00993321\n",
            " 0.03192098 0.00827801 0.00827801 0.00832588 0.00827801 0.00827801\n",
            " 0.00959743 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.0152929  0.03135877 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.01062156\n",
            " 0.00827801 0.00827801 0.01252573 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.03792021 0.00827801 0.03193249 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801 0.00827801\n",
            " 0.00827801 0.00827801 0.00827801 0.00827801] [0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.04796499 0.04579334 0.01162294 0.03855269 0.00862079 0.00793358\n",
            " 0.04256508 0.00793358 0.00793358 0.01255176 0.01517734 0.00793358\n",
            " 0.02082551 0.00793358 0.01735687 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.02146892 0.01206951 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.01199918 0.01114294 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358 0.00793358\n",
            " 0.00793358 0.00793358 0.00793358 0.00793358] [1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01402827 0.06588998 0.05932023 0.05810516 0.06546826 0.00763809\n",
            " 0.01925474 0.00926807 0.01011849 0.01448146 0.00991552 0.00732047\n",
            " 0.01117055 0.00732047 0.00732047 0.01099176 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.01362093 0.00732047 0.00732047\n",
            " 0.00848871 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047 0.00732047\n",
            " 0.00732047 0.00732047 0.00732047 0.00732047] [0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00815879 0.02140545 0.00815879 0.01237549 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.02500274 0.01439698\n",
            " 0.00815879 0.01821771 0.00815879 0.00815879 0.00815879 0.02124474\n",
            " 0.0089277  0.00815879 0.02145726 0.00815879 0.00815879 0.01626414\n",
            " 0.00893999 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.01727811 0.00815879\n",
            " 0.00815879 0.00815879 0.03332911 0.00815879 0.02962762 0.00815879\n",
            " 0.01911183 0.00815879 0.00815879 0.00815879 0.00815879 0.03739298\n",
            " 0.00815879 0.00968982 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879 0.00815879\n",
            " 0.00815879 0.00815879 0.00815879 0.00815879] [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.05401456 0.01751726 0.09115937 0.01225903 0.0100941  0.00731156\n",
            " 0.00731156 0.01954621 0.05446441 0.0286146  0.00971951 0.00841302\n",
            " 0.01257412 0.0145708  0.00851925 0.02325375 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.01379735 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156 0.00731156\n",
            " 0.00731156 0.00731156 0.00731156 0.00731156] [1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00883453 0.07941481 0.0084244  0.02191376 0.01518206 0.04543442\n",
            " 0.02192944 0.0084244  0.00868849 0.0084244  0.0084244  0.0098394\n",
            " 0.01378269 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0085583  0.0084244  0.0084244  0.01152822 0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.00894961 0.0084244  0.0103846  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.01106166 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244  0.0084244  0.0084244\n",
            " 0.0084244  0.0084244  0.0084244  0.0084244 ] [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00821459 0.02094299 0.01052683 0.01484549 0.00821459 0.00872537\n",
            " 0.01622725 0.00821459 0.00821459 0.00997889 0.01047867 0.04487509\n",
            " 0.00821459 0.01842803 0.00879658 0.00821459 0.00821459 0.00821459\n",
            " 0.04220732 0.00821459 0.00985174 0.00821459 0.00821459 0.00821459\n",
            " 0.00979165 0.00821459 0.00821459 0.01435778 0.00821459 0.01042686\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.01017207 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00946558 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.05630504 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459 0.00821459\n",
            " 0.00821459 0.00821459 0.00821459 0.00821459] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00765326 0.02944184 0.00765326 0.05815955 0.06128314 0.00827733\n",
            " 0.00765326 0.00937711 0.05969171 0.00765326 0.05188049 0.00765326\n",
            " 0.00858175 0.010282   0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.0118664  0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00921285 0.00765326 0.00765326 0.00765326\n",
            " 0.00845867 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326 0.00765326\n",
            " 0.00765326 0.00765326 0.00765326 0.00765326] [0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00818951 0.0655105  0.00818951 0.01323718 0.00921196 0.00818951\n",
            " 0.00818951 0.00832652 0.05818725 0.00818951 0.01592667 0.01004439\n",
            " 0.01730987 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.04309518 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.01828757 0.00818951 0.00818951 0.00818951\n",
            " 0.01199609 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951 0.00818951\n",
            " 0.00818951 0.00818951 0.00818951 0.00818951] [0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01310429 0.0302471  0.01175444 0.07765479 0.05817997 0.01109106\n",
            " 0.04442011 0.00792064 0.00792064 0.00792064 0.00910287 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00852928 0.00792064 0.00792064 0.02195036 0.00902926 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064 0.00792064\n",
            " 0.00792064 0.00792064 0.00792064 0.00792064] [0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00758172 0.07422021 0.01627919 0.0593488  0.06279596 0.04295174\n",
            " 0.02125004 0.00758172 0.01320075 0.00758172 0.00864928 0.00758172\n",
            " 0.00765827 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.0102076  0.00758172 0.00758172 0.00831731 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00792908 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172 0.00758172\n",
            " 0.00758172 0.00758172 0.00758172 0.00758172] [0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00761484 0.01266294 0.04737273 0.01631599 0.02139222 0.02583756\n",
            " 0.0149915  0.02171706 0.0106361  0.01592457 0.00761484 0.01449713\n",
            " 0.00761484 0.03829972 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.01301803 0.00761484 0.00761484 0.01138836 0.02272718 0.00761484\n",
            " 0.00774784 0.00761484 0.00761484 0.00988174 0.00761484 0.01448988\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00994611 0.00761484 0.00761484 0.01821906 0.00761484 0.00761484\n",
            " 0.00761484 0.04374738 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484 0.00761484\n",
            " 0.00761484 0.00761484 0.00761484 0.00761484] [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.06025061 0.02512507 0.05742536 0.02170356 0.01311383 0.00753711\n",
            " 0.01144411 0.0151753  0.00933052 0.01863974 0.00753711 0.00753711\n",
            " 0.04823769 0.00843265 0.0084302  0.04323607 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00963257 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00916798 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711 0.00753711\n",
            " 0.00753711 0.00753711 0.00753711 0.00753711] [1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.06134261 0.01620696 0.03237922 0.03628276 0.02035507 0.00799439\n",
            " 0.00978726 0.01096172 0.00799439 0.0346049  0.00799439 0.00799439\n",
            " 0.02075241 0.01311208 0.0091562  0.02062338 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00977402 0.00799439 0.00799439\n",
            " 0.00832911 0.00799439 0.00799439 0.00799439 0.00799439 0.00925125\n",
            " 0.00799439 0.01497126 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00857515 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439 0.00799439\n",
            " 0.00799439 0.00799439 0.00799439 0.00799439] [1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0241479  0.05636686 0.05841544 0.02091807 0.01917102 0.00846759\n",
            " 0.04524991 0.00804225 0.00773443 0.05154431 0.00985034 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00909153 0.00764275 0.00764275\n",
            " 0.01457588 0.00764275 0.00764275 0.00914761 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275 0.00764275\n",
            " 0.00764275 0.00764275 0.00764275 0.00764275] [0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00767204 0.06686655 0.01688715 0.03326593 0.06069587 0.05665499\n",
            " 0.0125814  0.01179468 0.02314021 0.00767204 0.01176924 0.01016046\n",
            " 0.01139575 0.00906117 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00825918 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204 0.00767204\n",
            " 0.00767204 0.00767204 0.00767204 0.00767204] [0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00814268 0.04107292 0.00814268 0.00814268 0.00814268 0.00827349\n",
            " 0.00814268 0.00814268 0.01065576 0.00814268 0.01839667 0.00905697\n",
            " 0.00814268 0.01644986 0.01084352 0.00814268 0.00814268 0.02521809\n",
            " 0.00814268 0.00814268 0.01711518 0.00958727 0.00814268 0.00834184\n",
            " 0.00842335 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00926846 0.00814268 0.03875093 0.00814268\n",
            " 0.00814268 0.00814268 0.04342568 0.00814268 0.0141468  0.00814268\n",
            " 0.02252214 0.00814268 0.00814268 0.00814268 0.00814268 0.02075139\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268 0.00814268\n",
            " 0.00814268 0.00814268 0.00814268 0.00814268] [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00778436 0.0634157  0.00778436 0.01794035 0.00932328 0.01134486\n",
            " 0.00778436 0.00786897 0.01247617 0.00778436 0.04127808 0.05406334\n",
            " 0.00790876 0.04753796 0.00778436 0.00778436 0.00778436 0.01069377\n",
            " 0.00934964 0.00778436 0.00778436 0.00778436 0.00778436 0.01167873\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00960667 0.00778436 0.00994109 0.00778436\n",
            " 0.00778436 0.00778436 0.00833761 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.01082534 0.00778436 0.01239413\n",
            " 0.00778436 0.01348201 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436 0.00778436\n",
            " 0.00778436 0.00778436 0.00778436 0.00778436] [0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01525458 0.05674868 0.04763429 0.04738217 0.01100818 0.00758757\n",
            " 0.01048624 0.00877702 0.0096873  0.01084224 0.00756926 0.00756926\n",
            " 0.03504617 0.00756926 0.03292527 0.03669659 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00758766 0.01475194 0.00756926 0.00756926\n",
            " 0.01176633 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926 0.00756926\n",
            " 0.00756926 0.00756926 0.00756926 0.00756926] [0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01364865 0.00847327 0.01299293 0.02824385 0.0372432  0.00800411\n",
            " 0.01626989 0.00800411 0.00800411 0.05540432 0.00800411 0.00800411\n",
            " 0.00800411 0.01815869 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.01556346 0.00800411 0.00800411 0.0103342  0.01038277 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.05361985\n",
            " 0.00800411 0.00947858 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.0184262  0.00800411 0.00800411 0.00839898 0.00800411 0.00800411\n",
            " 0.00800411 0.01101611 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411 0.00800411\n",
            " 0.00800411 0.00800411 0.00800411 0.00800411] [0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00801757 0.03555863 0.00881041 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.03928643 0.00801757 0.00801757 0.00801757\n",
            " 0.04589157 0.00801757 0.04361245 0.01121101 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.01100353 0.01526302 0.00801757 0.00801757\n",
            " 0.02158724 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00961701 0.00801757 0.01263139 0.00801757\n",
            " 0.00801757 0.00801757 0.03996669 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.0080319  0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757 0.00801757\n",
            " 0.00801757 0.00801757 0.00801757 0.00801757] [0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.05309353 0.01480455 0.00810274 0.05602067 0.00850011 0.00810274\n",
            " 0.01387275 0.00810274 0.00810274 0.0140789  0.00810274 0.00810274\n",
            " 0.01367838 0.0100878  0.01464717 0.01056949 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.010393   0.01584262 0.00828466 0.00810274\n",
            " 0.01021478 0.00810274 0.00810274 0.00810274 0.00810274 0.0172279\n",
            " 0.00810274 0.04313177 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.01302419 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274 0.00810274\n",
            " 0.00810274 0.00810274 0.00810274 0.00810274] [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00793664 0.10123495 0.01037307 0.02938771 0.01778799 0.06541813\n",
            " 0.01591103 0.00768214 0.01192215 0.00768214 0.01105211 0.01440824\n",
            " 0.02551898 0.01071207 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00947835 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00819485 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214 0.00768214\n",
            " 0.00768214 0.00768214 0.00768214 0.00768214] [0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.02131114 0.01312324 0.0522972  0.04385186 0.0480181  0.00898315\n",
            " 0.01318188 0.00973236 0.00742893 0.03463418 0.00742893 0.00742893\n",
            " 0.00742893 0.02001968 0.00742893 0.01373923 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.01938075 0.01473015 0.00742893\n",
            " 0.00977649 0.00742893 0.00742893 0.00742893 0.00742893 0.01389284\n",
            " 0.00742893 0.00887259 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.03707163 0.00742893 0.00742893 0.00821118 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893 0.00742893\n",
            " 0.00742893 0.00742893 0.00742893 0.00742893] [0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01136666 0.05880312 0.05568201 0.00877892 0.01115737 0.01048659\n",
            " 0.00716073 0.05387717 0.07627114 0.01192974 0.01002789 0.01674808\n",
            " 0.02744044 0.00797978 0.00809988 0.02041278 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00758442 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00901368 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073 0.00716073\n",
            " 0.00716073 0.00716073 0.00716073 0.00716073] [0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.008128   0.05592628 0.01288209 0.00924906 0.008128   0.008128\n",
            " 0.00964966 0.03094683 0.04701248 0.008128   0.01265707 0.04693824\n",
            " 0.01725915 0.00883125 0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.00962161 0.008128   0.01008067\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.01474749 0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.01489827 0.008128   0.008128\n",
            " 0.008128   0.00842    0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128   0.008128   0.008128\n",
            " 0.008128   0.008128   0.008128   0.008128  ] [0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00892283 0.04328816 0.01007399 0.01176129 0.00920447 0.00892283\n",
            " 0.02847233 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.01063917 0.00892283 0.00892283 0.01020999\n",
            " 0.00892283 0.00892283 0.00892283 0.05090187 0.0090404  0.00892283\n",
            " 0.01040388 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.01000352 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.01079181 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283 0.00892283\n",
            " 0.00892283 0.00892283 0.00892283 0.00892283] [0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0154085  0.07469204 0.00848459 0.05875953 0.02905783 0.01534454\n",
            " 0.06717391 0.00772158 0.00772158 0.00772158 0.0080363  0.00772158\n",
            " 0.01018738 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.01211871 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.01351596 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158 0.00772158\n",
            " 0.00772158 0.00772158 0.00772158 0.00772158] [0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0077475  0.05237157 0.01271908 0.02244857 0.04708212 0.04144482\n",
            " 0.04778295 0.0077475  0.0077475  0.01215535 0.0077475  0.0077475\n",
            " 0.00861422 0.01051511 0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.04634751 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.00899084\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475  0.0077475  0.0077475\n",
            " 0.0077475  0.0077475  0.0077475  0.0077475 ] [0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00758423 0.05995123 0.05943679 0.01700355 0.01057091 0.04918673\n",
            " 0.05417489 0.01266749 0.00758423 0.01108584 0.00758423 0.01077775\n",
            " 0.01331707 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.01384345 0.00758423 0.00758423 0.01751831 0.01025769 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00796498 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423 0.00758423\n",
            " 0.00758423 0.00758423 0.00758423 0.00758423] [0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.06556463 0.01616198 0.08144902 0.01581898 0.01457554 0.0073286\n",
            " 0.00870859 0.01576508 0.01251682 0.01404329 0.0073286  0.0073286\n",
            " 0.01748993 0.01097479 0.0073286  0.0389309  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.00993403 0.0073286  0.0073286\n",
            " 0.0454992  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.00963625 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286  0.0073286  0.0073286\n",
            " 0.0073286  0.0073286  0.0073286  0.0073286 ] [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00841108 0.05324363 0.00913488 0.01140597 0.00841108 0.00859127\n",
            " 0.00841108 0.01501938 0.02117262 0.00841108 0.01159073 0.05055317\n",
            " 0.01261801 0.02505298 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00849795 0.00841108 0.00841108 0.01157718\n",
            " 0.00841108 0.00841108 0.00841108 0.01014372 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.01022825 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.01125024 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.02117627 0.00841108 0.00844467\n",
            " 0.00841108 0.01059025 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108 0.00841108\n",
            " 0.00841108 0.00841108 0.00841108 0.00841108] [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00811932 0.01204136 0.00880247 0.01471871 0.00811932 0.00882703\n",
            " 0.01164414 0.00811932 0.00811932 0.00811932 0.00811932 0.01187138\n",
            " 0.00811932 0.01511857 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.0136388  0.05598073 0.03136156 0.01521365\n",
            " 0.04266958 0.00811932 0.00811932 0.00983452 0.00811932 0.0087011\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.0084805  0.00811932 0.02907131 0.00811932\n",
            " 0.01562425 0.00811932 0.00811932 0.00811932 0.00811932 0.01582235\n",
            " 0.00811932 0.01291339 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932 0.00811932\n",
            " 0.00811932 0.00811932 0.00811932 0.00811932] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00984527 0.04663428 0.01898921 0.02150741 0.01110702 0.00792638\n",
            " 0.00929435 0.00792638 0.0089446  0.00792638 0.00792638 0.00792638\n",
            " 0.02142702 0.00792638 0.04341605 0.01661754 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.0107422  0.05154895 0.00792638 0.00792638\n",
            " 0.03919617 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.0090614  0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638 0.00792638\n",
            " 0.00792638 0.00792638 0.00792638 0.00792638] [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00783015 0.04081802 0.00824147 0.04925301 0.05967217 0.01136043\n",
            " 0.00783015 0.00853631 0.04521594 0.00783015 0.04895118 0.00783015\n",
            " 0.00958911 0.011312   0.00783015 0.00783015 0.00783015 0.00800178\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00999578 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015 0.00783015\n",
            " 0.00783015 0.00783015 0.00783015 0.00783015] [0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01418484 0.09964406 0.01873986 0.0206017  0.01326292 0.00776608\n",
            " 0.00930928 0.0180905  0.01897381 0.00826512 0.01108515 0.01014161\n",
            " 0.08785658 0.00737752 0.01053531 0.01299406 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.01341423 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.01280059 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752 0.00737752\n",
            " 0.00737752 0.00737752 0.00737752 0.00737752] [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0235865  0.05521012 0.00811967 0.05463522 0.01008382 0.00933008\n",
            " 0.01827502 0.00811967 0.00811967 0.00811967 0.04877425 0.01510212\n",
            " 0.01034297 0.01508067 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.01077628 0.00811967 0.0082874  0.01000873 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00972748 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.01060728 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967 0.00811967\n",
            " 0.00811967 0.00811967 0.00811967 0.00811967] [0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.03922083 0.04982125 0.00989674 0.0480387  0.01009768 0.00808081\n",
            " 0.01409343 0.00808081 0.00808081 0.00808081 0.0121943  0.0088976\n",
            " 0.01211158 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.01032989 0.04614765 0.01265454 0.00808081\n",
            " 0.00908626 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00938969 0.01408763 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.01339056 0.00808081 0.00821019 0.00808081 0.00808081 0.00970503\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081 0.00808081\n",
            " 0.00808081 0.00808081 0.00808081 0.00808081] [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00990559 0.02737586 0.00778173 0.01631415 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.0127986  0.05940214\n",
            " 0.00795063 0.04294285 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00930999 0.00778173 0.00802619 0.012302   0.00778173 0.03982053\n",
            " 0.00818404 0.00778173 0.00778173 0.00796028 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.01165446 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.01524776 0.00778173\n",
            " 0.0115841  0.00778173 0.00778173 0.01083239 0.00778173 0.04447285\n",
            " 0.00778173 0.01359573 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173 0.00778173\n",
            " 0.00778173 0.00778173 0.00778173 0.00778173] [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01654278 0.00828103 0.00828103 0.04685551 0.01374309 0.00828103\n",
            " 0.00980264 0.00828103 0.00828103 0.01204033 0.00828103 0.00828103\n",
            " 0.00828103 0.01525758 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.01910083 0.01282305 0.00828103\n",
            " 0.00975505 0.00828103 0.00828103 0.00828103 0.00828103 0.05126183\n",
            " 0.00828103 0.01636639 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.04543806 0.00828103 0.00828103 0.01056291 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103 0.00828103\n",
            " 0.00828103 0.00828103 0.00828103 0.00828103] [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00824592 0.04387594 0.00824592 0.01315094 0.00824592 0.0112917\n",
            " 0.01538611 0.00824592 0.00824592 0.00824592 0.00950612 0.01217212\n",
            " 0.01220299 0.01738005 0.02144028 0.00824592 0.00824592 0.01103517\n",
            " 0.04148762 0.00824592 0.02573278 0.00937129 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.03777927 0.00824592\n",
            " 0.00824592 0.00824592 0.01126031 0.00824592 0.00928709 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.01322856 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592 0.00824592\n",
            " 0.00824592 0.00824592 0.00824592 0.00824592] [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00856919 0.04647679 0.01481688 0.05282276 0.05991105 0.00965254\n",
            " 0.01296393 0.02768292 0.01677952 0.00762696 0.00895552 0.00762696\n",
            " 0.05213348 0.00762696 0.00762696 0.00836884 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.01594209 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.009006   0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696 0.00762696\n",
            " 0.00762696 0.00762696 0.00762696 0.00762696] [0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.07604135 0.06425694 0.00774152 0.02114377 0.00834619 0.00774152\n",
            " 0.0144858  0.00774152 0.00774152 0.00774152 0.0132143  0.00913111\n",
            " 0.01289655 0.01031145 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00789978 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.06287611 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.01637671 0.00774152 0.00950769 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152 0.00774152\n",
            " 0.00774152 0.00774152 0.00774152 0.00774152] [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.02705589 0.06552839 0.06445929 0.02515152 0.06121974 0.00959561\n",
            " 0.03648805 0.01152057 0.00744405 0.01510797 0.00731613 0.00731613\n",
            " 0.01442303 0.00731613 0.00731613 0.01397949 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.01022515 0.00731613 0.0086137  0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613 0.00731613\n",
            " 0.00731613 0.00731613 0.00731613 0.00731613] [0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.04227041 0.02338812 0.01860467 0.02064465 0.03797488 0.00789677\n",
            " 0.00836246 0.04502923 0.01191492 0.00946388 0.00789677 0.00789677\n",
            " 0.05907273 0.01044525 0.00789677 0.00969516 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00960315 0.00895629 0.00789677\n",
            " 0.01065184 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00847435 0.00789677 0.01001599 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677 0.00789677\n",
            " 0.00789677 0.00789677 0.00789677 0.00789677] [1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01038175 0.076255   0.00971753 0.0203496  0.01506838 0.00751173\n",
            " 0.00751173 0.0198049  0.07162106 0.00751173 0.01207579 0.00911518\n",
            " 0.06032256 0.00751173 0.00963784 0.00853901 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.0205586  0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.01054358 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173 0.00751173\n",
            " 0.00751173 0.00751173 0.00751173 0.00751173] [0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01194986 0.05571039 0.03758317 0.01849488 0.00824288 0.00824288\n",
            " 0.03578344 0.010974   0.0098364  0.00824288 0.00824288 0.03767349\n",
            " 0.01304011 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00860048 0.00824288 0.00824288 0.01931539 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.01566487 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288 0.00824288\n",
            " 0.00824288 0.00824288 0.00824288 0.00824288] [0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00802062 0.00802062 0.01254356 0.02439432 0.01169833 0.03096222\n",
            " 0.01947147 0.00802062 0.00802062 0.00989363 0.00802062 0.00802062\n",
            " 0.00802062 0.0121632  0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00825223 0.00802062 0.00802062 0.04940804 0.03257689 0.00802062\n",
            " 0.01082315 0.00802062 0.00802062 0.00802062 0.00802062 0.04815469\n",
            " 0.00802062 0.00842988 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.02795688 0.00802062 0.00802062 0.01151876 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062 0.00802062\n",
            " 0.00802062 0.00802062 0.00802062 0.00802062] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00791412 0.06115686 0.01827033 0.01688234 0.0098612  0.03587154\n",
            " 0.01741423 0.0106945  0.01032503 0.00791412 0.00791412 0.00993547\n",
            " 0.06167454 0.00791412 0.01670141 0.02977489 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.01290944 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412 0.00791412\n",
            " 0.00791412 0.00791412 0.00791412 0.00791412] [0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.02335799 0.03182266 0.0082669  0.03862012 0.00812421 0.00812421\n",
            " 0.03200554 0.00812421 0.00812421 0.00970179 0.00812421 0.00812421\n",
            " 0.02761279 0.0081764  0.04408015 0.01030785 0.00812421 0.00812421\n",
            " 0.01282937 0.00812421 0.01970368 0.01353295 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00923784\n",
            " 0.00812421 0.0164465  0.00812421 0.00812421 0.01186384 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421 0.00812421\n",
            " 0.00812421 0.00812421 0.00812421 0.00812421] [0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.0475954  0.05518758 0.021078   0.02806963 0.06099166 0.00792616\n",
            " 0.04671676 0.00775414 0.00775414 0.01187314 0.00960556 0.00775414\n",
            " 0.00778554 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00785266 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.01087383 0.00775414 0.00983387 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414 0.00775414\n",
            " 0.00775414 0.00775414 0.00775414 0.00775414] [1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.01269823 0.06833152 0.01375949 0.01349961 0.00960476 0.01106426\n",
            " 0.02045791 0.01248176 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.09419758 0.00801948 0.01288535 0.01238465 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.01105111 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.0098884  0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948 0.00801948\n",
            " 0.00801948 0.00801948 0.00801948 0.00801948] [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00803561 0.07426579 0.00977105 0.02909635 0.01772582 0.04985914\n",
            " 0.01206336 0.00803561 0.01194915 0.00803561 0.01378833 0.03196666\n",
            " 0.01210628 0.01840381 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.0089845  0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00857806 0.00803561 0.00803561 0.01037877 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561 0.00803561\n",
            " 0.00803561 0.00803561 0.00803561 0.00803561] [0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0.00974488 0.06541468 0.04409364 0.02368202 0.00980665 0.00815855\n",
            " 0.02267373 0.01142455 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.04783627 0.00741923 0.01254087 0.0146735  0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00925762 0.07175311 0.00741923 0.00741923\n",
            " 0.0108856  0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923 0.00741923\n",
            " 0.00741923 0.00741923 0.00741923 0.00741923] [0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Uq1LzcvzrK7"
      },
      "source": [
        "## Remove previous checkpoints and add new ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwRtfGnfL9I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the this cell only when you want to start from scratch!\n",
        "!rm /rigel/home/sh3831/checkpoints/Final-ADL-project/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GSOQ6MyUzuSZ",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/rigel/home/sh3831/checkpoints/Final-ADL-project/'\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:08d}.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vsiDUVWB5B1s",
        "outputId": "f323fefa-dab3-4bb6-ed90-7a934e8ca8b4",
        "colab": {}
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "if latest != None:\n",
        "  print(\"Loading weights from\", latest)\n",
        "  model.load_weights(latest)\n",
        "else:\n",
        "  print(\"Checkpoint not found. Starting from scratch\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint not found. Starting from scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OPx91fcE07zc"
      },
      "source": [
        "## Accuracy and loss Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ll9_hjdx067u",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.BinaryCrossentropy(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JBk-CGh71IHE"
      },
      "source": [
        "## Train the model formally\n",
        "Now we train the model using all the training data available to us.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PALivwNF1LpU",
        "colab": {}
      },
      "source": [
        "def evaluate(max_steps=None):\n",
        "  steps = 0\n",
        "  for embedding_batch, label_batch in val_ds:\n",
        "    if max_steps != None and steps == max_steps:\n",
        "      break\n",
        "    predictions = model.predict(x=embedding_batch)\n",
        "    steps += 1 \n",
        "    # Record metrics after each batch\n",
        "    val_loss(label_batch, predictions)\n",
        "    val_accuracy(label_batch, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b049d162-8d34-4909-b537-f884b09aab8d",
        "id": "IVejbfyHectB",
        "colab": {}
      },
      "source": [
        "train_loss_history, train_acc_history = [], []\n",
        "val_loss_history, val_acc_history = [], []\n",
        "\n",
        "epochs = 20 # Your code here\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  # Train for one epoch\n",
        "  for embedding_batch, label_batch in train_ds:\n",
        "    result = model.train_on_batch(x=embedding_batch, y=label_batch)\n",
        "\n",
        "    # Record metrics after each batch\n",
        "    train_loss(result[0])\n",
        "    train_accuracy(result[1])\n",
        "\n",
        "  # Evaluate for a few steps\n",
        "  evaluate(max_steps=100)\n",
        "\n",
        "  # Print progress\n",
        "  # You should not need to modify this.\n",
        "  template = 'Epoch {}, Loss: {:.4f}, Accuracy: {:.4f}, Val Loss {:.4f}, Val Accuracy {:.4f}, Time: {:.1f} secs'\n",
        "  print(template.format(epoch,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        val_loss.result(),\n",
        "                        val_accuracy.result() * 100,\n",
        "                        time.time() - start))\n",
        "  \n",
        "  # Record history\n",
        "  train_loss_history.append(train_loss.result())\n",
        "  train_acc_history.append(train_accuracy.result() * 100)\n",
        "  val_loss_history.append(val_loss.result())\n",
        "  val_acc_history.append(val_accuracy.result() * 100)\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_accuracy.reset_states()\n",
        "\n",
        "  # Your code here\n",
        "  # Save a checkpoint after each epoch\n",
        "  print(\"Saving weights\")\n",
        "  model.save_weights(checkpoint_dir + 'biLSTM')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.1716, Accuracy: 96.1586, Val Loss 0.1675, Val Accuracy 96.0982, Time: 32.6 secs\n",
            "Saving weights\n",
            "Epoch 1, Loss: 0.1705, Accuracy: 96.1584, Val Loss 0.1642, Val Accuracy 96.0982, Time: 32.6 secs\n",
            "Saving weights\n",
            "Epoch 2, Loss: 0.1700, Accuracy: 96.1584, Val Loss 0.1657, Val Accuracy 96.0982, Time: 32.6 secs\n",
            "Saving weights\n",
            "Epoch 3, Loss: 0.1699, Accuracy: 96.1584, Val Loss 0.1668, Val Accuracy 96.0982, Time: 32.7 secs\n",
            "Saving weights\n",
            "Epoch 4, Loss: 0.1696, Accuracy: 96.1584, Val Loss 0.1654, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 5, Loss: 0.1696, Accuracy: 96.1588, Val Loss 0.1658, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 6, Loss: 0.1697, Accuracy: 96.1589, Val Loss 0.1647, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 7, Loss: 0.1693, Accuracy: 96.1586, Val Loss 0.1655, Val Accuracy 96.0982, Time: 32.6 secs\n",
            "Saving weights\n",
            "Epoch 8, Loss: 0.1692, Accuracy: 96.1588, Val Loss 0.1660, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 9, Loss: 0.1692, Accuracy: 96.1585, Val Loss 0.1654, Val Accuracy 96.0982, Time: 32.6 secs\n",
            "Saving weights\n",
            "Epoch 10, Loss: 0.1694, Accuracy: 96.1586, Val Loss 0.1653, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 11, Loss: 0.1691, Accuracy: 96.1585, Val Loss 0.1656, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 12, Loss: 0.1693, Accuracy: 96.1588, Val Loss 0.1658, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 13, Loss: 0.1693, Accuracy: 96.1586, Val Loss 0.1659, Val Accuracy 96.0982, Time: 32.4 secs\n",
            "Saving weights\n",
            "Epoch 14, Loss: 0.1691, Accuracy: 96.1589, Val Loss 0.1649, Val Accuracy 96.0982, Time: 32.4 secs\n",
            "Saving weights\n",
            "Epoch 15, Loss: 0.1691, Accuracy: 96.1585, Val Loss 0.1652, Val Accuracy 96.0982, Time: 32.4 secs\n",
            "Saving weights\n",
            "Epoch 16, Loss: 0.1689, Accuracy: 96.1590, Val Loss 0.1658, Val Accuracy 96.0982, Time: 32.4 secs\n",
            "Saving weights\n",
            "Epoch 17, Loss: 0.1687, Accuracy: 96.1587, Val Loss 0.1646, Val Accuracy 96.0982, Time: 32.4 secs\n",
            "Saving weights\n",
            "Epoch 18, Loss: 0.1690, Accuracy: 96.1585, Val Loss 0.1646, Val Accuracy 96.0982, Time: 32.5 secs\n",
            "Saving weights\n",
            "Epoch 19, Loss: 0.1690, Accuracy: 96.1587, Val Loss 0.1646, Val Accuracy 96.0982, Time: 32.4 secs\n",
            "Saving weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oaoLbwNZtXj",
        "colab_type": "text"
      },
      "source": [
        "**ALERT: For some reason, the plots didn't show up when we ran the code using HPC.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rqr-kbgl19ve",
        "colab": {}
      },
      "source": [
        "def plot(loss, acc, val_loss, val_acc):\n",
        "\n",
        "  # Get the number of epochs\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.plot(epochs, acc, color='blue', label='Train')\n",
        "  plt.plot(epochs, val_acc, color='orange', label='Val')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  _ = plt.figure()\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.plot(epochs, loss, color='blue', label='Train')\n",
        "  plt.plot(epochs, val_loss, color='orange', label='Val')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "plot(train_loss_history, train_acc_history, val_loss_history, val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBLEuzw0JOJK",
        "colab_type": "code",
        "outputId": "f99b22bf-c57f-4b87-be91-c346c62bfafb",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints\t\t    hello_world.cu  slurm-17182314.out\r\n",
            "cnn-dailymail\t\t    hello_world.sh  slurm-17194430.out\r\n",
            "Final_project_biLSTM.ipynb  http\t    slurm-17196758.out\r\n",
            "gpu.sh\t\t\t    setuptools\t    tensorflow-gpu\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8o-NIDHJOJL",
        "colab_type": "text"
      },
      "source": [
        "## Compare machine generated results with the human abstracts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btjPJYEpaPWl",
        "colab_type": "text"
      },
      "source": [
        "A fun way to evaluate how our model has performed is to compare the summary generated by the model to the actual summaries of articles put together by humans. \n",
        "\n",
        "We experimented with different summary lengths, but the accuracy suffered when it was more than 4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5o0pNiJOJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "human_abstract_path = '/rigel/home/sh3831/cnn-dailymail/human-abstracts/test/'\n",
        "test_embedding_path = '/rigel/home/sh3831/cnn-dailymail/sentence_embeddings/test/'\n",
        "summary_length = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pglb9vKbKoM",
        "colab_type": "text"
      },
      "source": [
        "#### Get embedding files for test data and create a random sample of 5.\n",
        "We show the comparison of machine summary of articles corresponding to these five embeddings and the human summaries for the same stories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnUkjUdfJOJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_embedding_files = os.listdir(test_embedding_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4t0qE4SJOJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples_for_test = random.sample(test_embedding_files, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QshAAtHFJOJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_input_batch, _ = fetch_data(examples_for_test, test_embedding_path, '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-EdN_mSJOJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_input_batch = tf.convert_to_tensor(example_input_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jeZKoeBJOJU",
        "colab_type": "code",
        "outputId": "83d41993-ee3a-4b1a-898c-8da78583fb5a",
        "colab": {}
      },
      "source": [
        "example_input_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 20, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXnRXXEtJOJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = model.predict(x=example_input_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke68sHc0JOJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_index = []\n",
        "for arr in predict:\n",
        "    arr = np.flip(arr.argsort())\n",
        "    predict_index.append(arr.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvbjYZAUbWJ2",
        "colab_type": "text"
      },
      "source": [
        "#### Function to compare the summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IomeX4aJJOJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_summary(summary_length, machine_summary, human_abstract, print_exp):\n",
        "    test_hypotheses = []\n",
        "    test_references = []\n",
        "    for index, file_name in enumerate(human_abstract):\n",
        "        hypothesis = ''\n",
        "        reference = ''\n",
        "        if print_exp == 1:\n",
        "            print('Machine Summary:')\n",
        "        with open(test_embedding_path + file_name) as embed_json_file:\n",
        "            embed_data = json.load(embed_json_file)\n",
        "            all_sentences = list(embed_data.keys())\n",
        "            if len(all_sentences) < summary_length:\n",
        "                length = len(all_sentences)\n",
        "            else:\n",
        "                length = summary_length\n",
        "            for i in machine_summary[index][:length]:\n",
        "                if i < len(all_sentences):\n",
        "                    if print_exp == 1:\n",
        "                        print(all_sentences[i])\n",
        "                    hypothesis += all_sentences[i]\n",
        "        if print_exp == 1:\n",
        "            print('Human Abstract:')\n",
        "        f = open(human_abstract_path + file_name.replace('.json', '.spl'), \"r\")\n",
        "        for sentence in f.readlines():\n",
        "            if print_exp == 1:\n",
        "                print(sentence.replace('\\n',''))\n",
        "            reference += sentence.replace('\\n','')\n",
        "        if print_exp == 1:\n",
        "            print('---------------------')\n",
        "        test_hypotheses.append(hypothesis)\n",
        "        test_references.append(reference)\n",
        "    \n",
        "    return test_hypotheses, test_references"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dsTHOgwbfD5",
        "colab_type": "text"
      },
      "source": [
        "#### Printing a collection of machine-generated summaries and human abstracts for the same articles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onBbsu6qWbTI",
        "colab_type": "code",
        "outputId": "b948bcf1-f640-488d-bf5e-74217fb61c68",
        "colab": {}
      },
      "source": [
        "test_hypotheses, test_references = compare_summary(summary_length, predict_index, examples_for_test, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Machine Summary:\n",
            "And like Oskar Matzerath , the boy in \" The Tin Drum , \" they often slip into surreal situations .\n",
            "\" In his excavation of the past , Günter Grass goes deeper than most and he unearths the intertwined roots of good and evil , \" the Nobel committee wrote , when it awarded him the literature prize in 1999 .\n",
            "\" The Tin Drum , \" which was published in 1959 , \" breaks the bounds of realism by having as its protagonist and narrator an infernal intelligence in the body of a three-year-old , a monster who overpowers the fellow human beings he approaches with the help of a toy drum , \" the Nobel committee wrote .\n",
            "A year later , he penned a detailed account in The New Yorker on how he spent his war years up to the death of German dictator Adolf Hitler .\n",
            "Human Abstract:\n",
            "Grass tried in his literature to come to grips with World War II and the Nazi era .\n",
            "His characters were the downtrodden , and his style slipped into the surreal .\n",
            "He stoked controversy with his admission to being a member of the Waffen SS .\n",
            "---------------------\n",
            "Machine Summary:\n",
            "Lord Neuberger said Britain 's privileged judges must be aware of their ` subconscious bias ' when dealing with poorer members of society .\n",
            "` Would you feel that you had given of your best if you had been forced to give evidence in unfamiliar surroundings , with lots of strangers watching , in an intimidating court , with lawyers in funny clothes asking questions , often aggressively and trying to catch you out , and with no ability to tell the story as you remember it ? '\n",
            "In a lengthy speech entitled ` Fairness in the courts : the best we can do ' , Lord Neuberger also said judges and lawyers should always keep in mind how ` intimidating ' the court process can be for those involved in trials , including ` the parties , their families , the victims , the witnesses and the jurors ' .\n",
            "` My concern is not theoretical ; it is now 18 months since a judge at Blackfriars Magistrate Court wasted a great deal of court time dealing with the question of full-face veils , and made a heartfelt plea for central guidance to avoid this inefficient use of expensive court 's resources being replicated elsewhere .\n",
            "Human Abstract:\n",
            "Lord Neuberger said Muslim women should be allowed to wear a full-face veil when appearing in court to show respect to ` different customs .\n",
            "He also said judges must be aware of their ` subconscious bias '\n",
            "Judges are ` rightly ' seen as from ` privileged ' part of society , he said .\n",
            "He cited a judge ruling on a case of an unemployed traveller as an example .\n",
            "Since publication of this article , Lord Neuberger has since clarified his position which may be read here : Article .\n",
            "---------------------\n",
            "Machine Summary:\n",
            "A father has told how he 's embraced 21st century parenting by using a drone to walk his eight-year-old daughter to school .\n",
            "Scroll down for video .\n",
            "Aerial views : Chris Early , who owns a production company in Knoxville , Tennessee , bought a quadcopter to make commercials but he decided he could use it for personal surveillance purposes too .\n",
            "' I trust her enough to give her that little bit of independence as she continues to grow into a smart , beautiful young lady .\n",
            "Human Abstract:\n",
            "Chris Early , who owns a production company in Knoxville , Tennessee , bought a quadcopter to make commercials .\n",
            "However , he decided he could use it for personal monitoring purposes too .\n",
            "Early released footage of one of his remote chaperoning sessions .\n",
            "The clip shows how he is able monitor his child 's location from the sky .\n",
            "---------------------\n",
            "Machine Summary:\n",
            "The main reasons for this were said to be ` showing off in front of friends ' and ` succumbing to peer pressure and buying things we would n't ordinary buy ' .\n",
            "` It 's all about priorities , ' said Lindsey Burgess , a spokesperson for You\n",
            "A further 19 per cent of respondents also said their spouse or partner gives them a hard time for overspending , though five per cent do n't even tell their significant others about their big purchases .\n",
            "Top items on which people overspend on include food ( 58 per cent ) , clothing ( 54 per cent ) , household items ( 28 per cent ) , shoes ( 21 per cent ) , and entertainment ( 20 per cent ) .\n",
            "Human Abstract:\n",
            "A new YouNeedaBudget.com study found that 64 per cent of adults spend more money with friends due to peer pressure or the desire to show off .\n",
            "The top items Americans overspend on are food and clothing .\n",
            "Five per cent of those polled said they hide big purchases from their spouses or significant others .\n",
            "---------------------\n",
            "Machine Summary:\n",
            "Although he had never run a marathon before , Callum Ryan , 21 , committed himself to the daunting challenge of running eight of them between January and September 2015 - covering a total distance of 337.5 kilometres .\n",
            "In his fourteen short years , Malachy left his mark on the world -- creating , daydreaming and making people smile .\n",
            "Malachy Frawley brought laughter to the world in his short but wonderful life , showing great courage through his lifelong battle with the severe heart condition Hypoplastic Right Heart Syndrome .\n",
            "When Malachy was in kindergarten the pair were excited to discover Callum had also been allocated as Malachy 's Year Six buddy -- a mentor and friendly face across the playground .\n",
            "Human Abstract:\n",
            "Callum Ryan , 21 , will run eight marathons in each of Australia 's states and territories between January and September this year .\n",
            "Callum is running to honour the memory of his late friend , Malachy Frawley .\n",
            "Malachy passed away when he was just 14 from a Congenital Heart Disease .\n",
            "Callum wants to raise funds for HeartKids Australia and awareness of the impact of children 's heart disease .\n",
            "He had never run a marathon before when he set himself the challenge .\n",
            "---------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l12Lr9cwWbTM",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model with Rouge scores\n",
        "Here we have shown the final scores with MAX_SEQ_LEN set to 100 and summary length set to 4. As discussed earlier, we tried different values for these two values. Bigger values for MAX_SEQ_LEN will lead to padding of all sentences. eg; we'll rarely find sentences 500 or 1000 words long. If the padded length is much larger than the original length, the accuracy of the model is influenced more by the prediction on the padded part, which does not add to real accuracy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The human abstracts are usually four sentences for each article, so intuitively, the model's summary should not be longer than that. \n",
        "Rouge scores have two parts: precision and recall. Precision is based on the length of the generated senetences and recall is based on human abstracts. Rouge scores use n-grams and longest common subsequences to evaluate the performance of the summary. If the generated sentences are too long then the precision score would be low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUZGRrIIWbTK",
        "colab_type": "code",
        "outputId": "1e8bc69d-3bc7-4d2d-edd2-9aec18722599",
        "colab": {}
      },
      "source": [
        "rouge = Rouge()\n",
        "for i in range(len(test_hypotheses)):\n",
        "    scores = rouge.get_scores(test_hypotheses[i], test_references[i])\n",
        "    print(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'rouge-l': {'r': 0.2857142857142857, 'f': 0.1125579580229207, 'p': 0.10416666666666667}, 'rouge-2': {'r': 0.023255813953488372, 'f': 0.011363632671101406, 'p': 0.007518796992481203}, 'rouge-1': {'r': 0.37142857142857144, 'f': 0.19847327852689245, 'p': 0.13541666666666666}}]\n",
            "[{'rouge-l': {'r': 0.3582089552238806, 'f': 0.20551829777223987, 'p': 0.18461538461538463}, 'rouge-2': {'r': 0.1744186046511628, 'f': 0.10600706290701614, 'p': 0.07614213197969544}, 'rouge-1': {'r': 0.44776119402985076, 'f': 0.30456852343013224, 'p': 0.23076923076923078}}]\n",
            "[{'rouge-l': {'r': 0.6041666666666666, 'f': 0.45465023703087476, 'p': 0.4084507042253521}, 'rouge-2': {'r': 0.48214285714285715, 'f': 0.3857142809142858, 'p': 0.32142857142857145}, 'rouge-1': {'r': 0.6041666666666666, 'f': 0.48739495316997394, 'p': 0.4084507042253521}}]\n",
            "[{'rouge-l': {'r': 0.3877551020408163, 'f': 0.25914887065583303, 'p': 0.23170731707317074}, 'rouge-2': {'r': 0.13725490196078433, 'f': 0.0853658493731412, 'p': 0.061946902654867256}, 'rouge-1': {'r': 0.4489795918367347, 'f': 0.3358778579127091, 'p': 0.2682926829268293}}]\n",
            "[{'rouge-l': {'r': 0.45161290322580644, 'f': 0.34227275155969855, 'p': 0.3076923076923077}, 'rouge-2': {'r': 0.17721518987341772, 'f': 0.14213197489139134, 'p': 0.11864406779661017}, 'rouge-1': {'r': 0.4838709677419355, 'f': 0.3921568579247299, 'p': 0.32967032967032966}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr9dvdIuWbTN",
        "colab_type": "code",
        "outputId": "8606c871-94d1-4ee3-889c-9d03d091ad39",
        "colab": {}
      },
      "source": [
        "embedding_test, _ = fetch_data(test_embedding_files, test_embedding_path, '')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITYfffYLWbTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_test = tf.convert_to_tensor(embedding_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXhU-5LmWbTR",
        "colab_type": "code",
        "outputId": "2e323a13-cba0-4313-d66a-0935ad82b2c5",
        "colab": {}
      },
      "source": [
        "embedding_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5000, 20, 200])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sHaikweWbTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = model.predict(x=embedding_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XyY8nvkWbTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_predict_indexes = []\n",
        "for arr in predict:\n",
        "    arr = np.flip(arr.argsort())\n",
        "    all_predict_indexes.append(arr.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLtV5GbiWbTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_hypotheses, test_references = compare_summary(summary_length, all_predict_indexes, test_embedding_files, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Rrct5eWbTp",
        "colab_type": "code",
        "outputId": "b2b545ca-7439-4986-a762-c24cb0d4ee94",
        "colab": {}
      },
      "source": [
        "scores = rouge.get_scores(test_hypotheses, test_references, avg=True)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-l': {'r': 0.3184194203254402, 'f': 0.22953341244998465, 'p': 0.22218828247868158}, 'rouge-2': {'r': 0.10893271407808798, 'f': 0.08074033546205592, 'p': 0.06963972601428428}, 'rouge-1': {'r': 0.35098872877184456, 'f': 0.2781814447736596, 'p': 0.2451894978343283}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}